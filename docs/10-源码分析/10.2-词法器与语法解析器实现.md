# 词法器与语法解析器实现

> **文档类型**: 源码实现分析  
> **核心模块**: `ast/parser.go`, `ast/scanner.go`  
> **适用读者**: 编译器开发者、语言设计者、深度学习者  
> **先修知识**: [Rego语法规范](../02-语言模型/02.1-Rego语法规范.md)、编译原理基础  
> **最后更新**: 2025年10月23日  
> **文档状态**: ✅ Phase 2.2 - 解析器实现  
> **OPA版本**: v0.68.0

---

## 🎯 实现分析说明

> **本文档目标**:
>
> - ✅ 深入理解OPA词法分析器的实现原理
> - ✅ 掌握手写递归下降解析器的设计模式
> - ✅ 分析Rego语法解析的关键技术点
> - ✅ 学习错误恢复和错误报告机制
>
> **技术亮点**:
>
> - **手写Scanner**: 高效的字符流处理
> - **递归下降Parser**: 清晰的语法结构映射
> - **错误恢复**: 生产级的错误处理
> - **位置追踪**: 精确的源码定位
>
> **实战价值**:
>
> - 学习编译器前端设计
> - 理解DSL解析实现
> - 掌握错误处理最佳实践
> - 为语言扩展打基础

---

## 目录

- [词法器与语法解析器实现](#词法器与语法解析器实现)
  - [🎯 实现分析说明](#-实现分析说明)
  - [目录](#目录)
  - [1. 解析器架构总览](#1-解析器架构总览)
    - [1.1 整体架构](#11-整体架构)
    - [1.2 核心数据结构](#12-核心数据结构)
    - [1.3 解析流程](#13-解析流程)
  - [2. 词法分析器（Scanner）](#2-词法分析器scanner)
    - [2.1 Scanner结构](#21-scanner结构)
    - [2.2 Token定义](#22-token定义)
    - [2.3 词法扫描实现](#23-词法扫描实现)
    - [2.4 关键字识别](#24-关键字识别)
    - [2.5 字符串与转义](#25-字符串与转义)
    - [2.6 数字解析](#26-数字解析)
  - [3. 语法解析器（Parser）](#3-语法解析器parser)
    - [3.1 Parser结构](#31-parser结构)
    - [3.2 递归下降解析](#32-递归下降解析)
    - [3.3 模块解析](#33-模块解析)
    - [3.4 规则解析](#34-规则解析)
    - [3.5 表达式解析](#35-表达式解析)
    - [3.6 项（Term）解析](#36-项term解析)
  - [4. 运算符优先级](#4-运算符优先级)
    - [4.1 优先级表](#41-优先级表)
    - [4.2 表达式解析算法](#42-表达式解析算法)
    - [4.3 Pratt Parsing](#43-pratt-parsing)
  - [5. 错误处理](#5-错误处理)
    - [5.1 错误类型](#51-错误类型)
    - [5.2 错误恢复策略](#52-错误恢复策略)
    - [5.3 错误报告](#53-错误报告)
    - [5.4 位置追踪](#54-位置追踪)
  - [6. 特殊语法处理](#6-特殊语法处理)
    - [6.1 集合推导式](#61-集合推导式)
    - [6.2 对象推导式](#62-对象推导式)
    - [6.3 数组推导式](#63-数组推导式)
    - [6.4 引用（Reference）](#64-引用reference)
    - [6.5 函数调用](#65-函数调用)
  - [7. 源码实现分析](#7-源码实现分析)
    - [7.1 Scanner核心代码](#71-scanner核心代码)
    - [7.2 Parser核心代码](#72-parser核心代码)
    - [7.3 性能优化](#73-性能优化)
  - [8. 实战示例](#8-实战示例)
    - [8.1 简单规则解析](#81-简单规则解析)
    - [8.2 复杂表达式解析](#82-复杂表达式解析)
    - [8.3 错误处理示例](#83-错误处理示例)
  - [9. 测试与验证](#9-测试与验证)
    - [9.1 单元测试](#91-单元测试)
    - [9.2 性能测试](#92-性能测试)
    - [9.3 Fuzzing测试](#93-fuzzing测试)
  - [附录](#附录)
    - [A. Token完整列表](#a-token完整列表)
    - [B. 语法规则BNF](#b-语法规则bnf)
    - [C. 调试工具](#c-调试工具)

---

## 1. 解析器架构总览

### 1.1 整体架构

OPA的解析器采用**两阶段设计**：

```text
┌─────────────────────────────────────────────┐
│           Rego源代码 (字符串)              │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│         阶段1: 词法分析 (Scanner)           │
│  ┌────────────────────────────────────┐    │
│  │  字符流 → Token流                  │    │
│  │  - 跳过空白                         │    │
│  │  - 识别关键字                       │    │
│  │  - 解析字面量                       │    │
│  │  - 位置追踪                         │    │
│  └────────────────────────────────────┘    │
└─────────────────────────────────────────────┘
                    ↓
       Token流: [PACKAGE, IDENT, ...]
                    ↓
┌─────────────────────────────────────────────┐
│         阶段2: 语法分析 (Parser)            │
│  ┌────────────────────────────────────┐    │
│  │  Token流 → AST                      │    │
│  │  - 递归下降解析                     │    │
│  │  - 构建AST节点                      │    │
│  │  - 语法验证                         │    │
│  │  - 错误恢复                         │    │
│  └────────────────────────────────────┘    │
└─────────────────────────────────────────────┘
                    ↓
          AST (抽象语法树)
```

### 1.2 核心数据结构

**Scanner状态**:

```go
// scanner: 词法分析器
type scanner struct {
    source   []byte    // 源代码
    offset   int       // 当前位置
    ch       rune      // 当前字符
    lineNum  int       // 行号
    linePos  int       // 行内位置
    
    // 错误收集
    errors   Errors
}
```

**Parser状态**:

```go
// parser: 语法分析器
type parser struct {
    s        *scanner  // 词法分析器
    tok      token     // 当前token
    tokVal   string    // token值
    
    // AST构建
    module   *Module   // 当前模块
    
    // 错误恢复
    panicMode bool
    errors    Errors
}
```

**Token定义**:

```go
// token: 词法单元类型
type token int

const (
    // 特殊token
    ILLEGAL token = iota
    EOF
    COMMENT
    
    // 字面量
    IDENT      // user, allow
    NUMBER     // 123, 3.14
    STRING     // "hello"
    
    // 关键字
    PACKAGE    // package
    IMPORT     // import
    AS         // as
    DEFAULT    // default
    IF         // if
    ELSE       // else
    
    // 运算符
    ASSIGN     // :=
    EQ         // ==
    NEQ        // !=
    LT         // <
    GT         // >
    
    // 分隔符
    LPAREN     // (
    RPAREN     // )
    LBRACK     // [
    RBRACK     // ]
    LBRACE     // {
    RBRACE     // }
    // ...
)
```

### 1.3 解析流程

**完整解析流程**:

```go
// ParseModule: 解析Rego模块
func ParseModule(filename string, input []byte) (*Module, error) {
    // 1. 创建scanner
    s := newScanner(input)
    
    // 2. 创建parser
    p := newParser(s)
    
    // 3. 解析模块
    module := p.parseModule()
    
    // 4. 检查错误
    if len(p.errors) > 0 {
        return nil, p.errors
    }
    
    return module, nil
}
```

---

## 2. 词法分析器（Scanner）

### 2.1 Scanner结构

**完整实现**:

```go
// scanner: 词法分析器
type scanner struct {
    // 输入源
    source   []byte
    filename string
    
    // 扫描状态
    offset   int      // 当前字节偏移
    readOff  int      // 下一个读取位置
    lineNum  int      // 当前行号
    linePos  int      // 行内位置
    ch       rune     // 当前字符
    
    // 错误
    errors   Errors
}

// newScanner: 创建scanner
func newScanner(source []byte) *scanner {
    s := &scanner{
        source:  source,
        lineNum: 1,
        linePos: 1,
    }
    s.next()  // 读取第一个字符
    return s
}
```

### 2.2 Token定义

**完整Token枚举**:

```go
const (
    // 特殊
    ILLEGAL token = iota
    EOF
    COMMENT
    WHITESPACE
    
    literal_beg
    IDENT      // user
    NUMBER     // 123
    STRING     // "hello"
    RAWSTRING  // `raw`
    literal_end
    
    keyword_beg
    AS
    DEFAULT
    ELSE
    FALSE
    IF
    IMPORT
    NOT
    NULL
    PACKAGE
    TRUE
    WITH
    SOME
    keyword_end
    
    operator_beg
    ASSIGN     // :=
    EQ         // ==
    NEQ        // !=
    LT         // <
    LTE        // <=
    GT         // >
    GTE        // >=
    PLUS       // +
    MINUS      // -
    MUL        // *
    DIV        // /
    MOD        // %
    AND        // &
    OR         // |
    operator_end
    
    LPAREN     // (
    RPAREN     // )
    LBRACK     // [
    RBRACK     // ]
    LBRACE     // {
    RBRACE     // }
    COMMA      // ,
    DOT        // .
    SEMICOLON  // ;
    COLON      // :
)
```

### 2.3 词法扫描实现

**核心扫描函数**:

```go
// scan: 扫描下一个token
func (s *scanner) scan() (tok token, lit string, pos Pos) {
    // 跳过空白
    s.skipWhitespace()
    
    // 记录位置
    pos = s.position()
    
    // 读取当前字符
    ch := s.ch
    
    switch {
    case isLetter(ch):
        // 标识符或关键字
        lit = s.scanIdentifier()
        tok = lookupKeyword(lit)
        return
        
    case isDigit(ch):
        // 数字
        tok, lit = s.scanNumber()
        return
        
    case ch == '"':
        // 字符串
        tok, lit = s.scanString()
        return
        
    case ch == '`':
        // 原始字符串
        tok, lit = s.scanRawString()
        return
    }
    
    // 运算符和分隔符
    s.next()  // 消费当前字符
    
    switch ch {
    case -1:
        tok = EOF
        
    case '(':
        tok = LPAREN
        
    case ')':
        tok = RPAREN
        
    case '[':
        tok = LBRACK
        
    case ']':
        tok = RBRACK
        
    case '{':
        tok = LBRACE
        
    case '}':
        tok = RBRACE
        
    case ',':
        tok = COMMA
        
    case '.':
        tok = DOT
        
    case ':':
        if s.ch == '=' {
            s.next()
            tok = ASSIGN  // :=
        } else {
            tok = COLON
        }
        
    case '=':
        if s.ch == '=' {
            s.next()
            tok = EQ  // ==
        } else {
            tok = ASSIGN
        }
        
    case '!':
        if s.ch == '=' {
            s.next()
            tok = NEQ  // !=
        } else {
            tok = NOT
        }
        
    case '<':
        if s.ch == '=' {
            s.next()
            tok = LTE  // <=
        } else {
            tok = LT
        }
        
    case '>':
        if s.ch == '=' {
            s.next()
            tok = GTE  // >=
        } else {
            tok = GT
        }
        
    case '+':
        tok = PLUS
        
    case '-':
        tok = MINUS
        
    case '*':
        tok = MUL
        
    case '/':
        tok = DIV
        
    case '%':
        tok = MOD
        
    case '#':
        // 注释
        lit = s.scanComment()
        tok = COMMENT
        
    default:
        s.error("illegal character")
        tok = ILLEGAL
    }
    
    return
}
```

**辅助函数**:

```go
// next: 读取下一个字符
func (s *scanner) next() {
    if s.readOff < len(s.source) {
        s.offset = s.readOff
        if s.ch == '\n' {
            s.lineNum++
            s.linePos = 1
        } else {
            s.linePos++
        }
        r, w := rune(s.source[s.readOff]), 1
        if r >= utf8.RuneSelf {
            r, w = utf8.DecodeRune(s.source[s.readOff:])
        }
        s.readOff += w
        s.ch = r
    } else {
        s.offset = len(s.source)
        s.ch = -1  // EOF
    }
}

// skipWhitespace: 跳过空白字符
func (s *scanner) skipWhitespace() {
    for s.ch == ' ' || s.ch == '\t' || s.ch == '\n' || s.ch == '\r' {
        s.next()
    }
}

// isLetter: 判断是否是字母
func isLetter(ch rune) bool {
    return 'a' <= ch && ch <= 'z' ||
           'A' <= ch && ch <= 'Z' ||
           ch == '_'
}

// isDigit: 判断是否是数字
func isDigit(ch rune) bool {
    return '0' <= ch && ch <= '9'
}
```

### 2.4 关键字识别

**关键字查找表**:

```go
var keywords = map[string]token{
    "as":      AS,
    "default": DEFAULT,
    "else":    ELSE,
    "false":   FALSE,
    "if":      IF,
    "import":  IMPORT,
    "not":     NOT,
    "null":    NULL,
    "package": PACKAGE,
    "some":    SOME,
    "true":    TRUE,
    "with":    WITH,
}

// lookupKeyword: 查找关键字
func lookupKeyword(ident string) token {
    if tok, ok := keywords[ident]; ok {
        return tok
    }
    return IDENT
}

// scanIdentifier: 扫描标识符
func (s *scanner) scanIdentifier() string {
    start := s.offset
    for isLetter(s.ch) || isDigit(s.ch) {
        s.next()
    }
    return string(s.source[start:s.offset])
}
```

### 2.5 字符串与转义

**字符串扫描**:

```go
// scanString: 扫描双引号字符串
func (s *scanner) scanString() (token, string) {
    // 消费开始的 "
    s.next()
    
    var buf bytes.Buffer
    start := s.offset
    
    for {
        ch := s.ch
        
        if ch == '\n' || ch < 0 {
            s.error("string literal not terminated")
            return ILLEGAL, ""
        }
        
        s.next()
        
        if ch == '"' {
            // 字符串结束
            break
        }
        
        if ch == '\\' {
            // 转义序列
            if buf.Len() == 0 {
                // 第一次遇到转义，先复制之前的内容
                buf.Write(s.source[start : s.offset-1])
            }
            
            ch = s.ch
            s.next()
            
            switch ch {
            case 'n':
                buf.WriteByte('\n')
            case 't':
                buf.WriteByte('\t')
            case 'r':
                buf.WriteByte('\r')
            case '"':
                buf.WriteByte('"')
            case '\\':
                buf.WriteByte('\\')
            case 'u':
                // Unicode转义: \u1234
                r := s.scanUnicodeEscape(4)
                buf.WriteRune(r)
            case 'U':
                // Unicode转义: \U12345678
                r := s.scanUnicodeEscape(8)
                buf.WriteRune(r)
            default:
                s.error("unknown escape sequence")
            }
        } else if buf.Len() > 0 {
            // 已经有buffer了，继续追加
            buf.WriteRune(ch)
        }
    }
    
    if buf.Len() > 0 {
        return STRING, buf.String()
    }
    return STRING, string(s.source[start : s.offset-1])
}

// scanUnicodeEscape: 扫描Unicode转义
func (s *scanner) scanUnicodeEscape(length int) rune {
    var val rune
    for i := 0; i < length; i++ {
        ch := s.ch
        s.next()
        
        var d rune
        switch {
        case '0' <= ch && ch <= '9':
            d = ch - '0'
        case 'a' <= ch && ch <= 'f':
            d = ch - 'a' + 10
        case 'A' <= ch && ch <= 'F':
            d = ch - 'A' + 10
        default:
            s.error("illegal character in escape sequence")
            return 0
        }
        val = val*16 + d
    }
    return val
}

// scanRawString: 扫描原始字符串（反引号）
func (s *scanner) scanRawString() (token, string) {
    // 消费开始的 `
    s.next()
    
    start := s.offset
    
    for {
        ch := s.ch
        
        if ch < 0 {
            s.error("raw string literal not terminated")
            return ILLEGAL, ""
        }
        
        s.next()
        
        if ch == '`' {
            break
        }
    }
    
    return RAWSTRING, string(s.source[start : s.offset-1])
}
```

### 2.6 数字解析

**数字扫描实现**:

```go
// scanNumber: 扫描数字
func (s *scanner) scanNumber() (token, string) {
    start := s.offset
    
    // 处理负号
    if s.ch == '-' {
        s.next()
    }
    
    // 整数部分
    s.scanDigits()
    
    tok := NUMBER
    
    // 小数部分
    if s.ch == '.' {
        s.next()
        s.scanDigits()
    }
    
    // 指数部分
    if s.ch == 'e' || s.ch == 'E' {
        s.next()
        if s.ch == '+' || s.ch == '-' {
            s.next()
        }
        s.scanDigits()
    }
    
    return tok, string(s.source[start:s.offset])
}

// scanDigits: 扫描数字序列
func (s *scanner) scanDigits() {
    for isDigit(s.ch) {
        s.next()
    }
}
```

---

## 3. 语法解析器（Parser）

### 3.1 Parser结构

**Parser实现**:

```go
// parser: 语法解析器
type parser struct {
    // 词法分析器
    s *scanner
    
    // 当前token
    tok    token
    tokVal string
    tokPos Pos
    
    // 构建中的AST
    module *Module
    
    // 错误处理
    errors    Errors
    panicMode bool
    
    // 上下文
    inRule bool  // 是否在规则内部
}

// newParser: 创建parser
func newParser(s *scanner) *parser {
    p := &parser{s: s}
    p.next()  // 读取第一个token
    return p
}

// next: 获取下一个token
func (p *parser) next() {
    p.tok, p.tokVal, p.tokPos = p.s.scan()
}

// expect: 期望特定token
func (p *parser) expect(tok token) {
    if p.tok != tok {
        p.errorf("expected %s, got %s", tok, p.tok)
    }
    p.next()
}

// match: 匹配并消费token
func (p *parser) match(tok token) bool {
    if p.tok == tok {
        p.next()
        return true
    }
    return false
}
```

### 3.2 递归下降解析

**递归下降方法**:

每个语法规则对应一个解析函数：

```text
语法规则                 →  解析函数
─────────────────────────────────────
Module                  →  parseModule()
Rule                    →  parseRule()
RuleHead                →  parseRuleHead()
RuleBody                →  parseRuleBody()
Expr                    →  parseExpr()
Term                    →  parseTerm()
```

**示例语法规则**:

```text
Module    = Package Imports* Rules*
Rule      = RuleHead RuleBody?
RuleHead  = Ref (ASSIGN | LPAREN)? Term*
RuleBody  = IF Body
Body      = Expr (SEMICOLON Expr)*
Expr      = Term | Expr OP Expr | NOT Expr
Term      = Ref | Var | Scalar | Array | Object | Set | Comprehension
```

### 3.3 模块解析

**parseModule实现**:

```go
// parseModule: 解析Rego模块
func (p *parser) parseModule() *Module {
    module := &Module{
        Location: p.tokPos.Location,
    }
    p.module = module
    
    // 1. 解析 package 声明
    module.Package = p.parsePackage()
    
    // 2. 解析 import 语句
    for p.tok == IMPORT {
        imp := p.parseImport()
        module.Imports = append(module.Imports, imp)
    }
    
    // 3. 解析规则
    for p.tok != EOF {
        if rule := p.parseRule(); rule != nil {
            module.Rules = append(module.Rules, rule)
        }
    }
    
    return module
}

// parsePackage: 解析package声明
func (p *parser) parsePackage() *Package {
    // package <ref>
    p.expect(PACKAGE)
    
    pkg := &Package{
        Location: p.tokPos.Location,
        Path:     p.parseRef(),
    }
    
    return pkg
}

// parseImport: 解析import语句
func (p *parser) parseImport() *Import {
    // import <ref> [as <var>]
    p.expect(IMPORT)
    
    imp := &Import{
        Location: p.tokPos.Location,
        Path:     p.parseRef(),
    }
    
    if p.match(AS) {
        imp.Alias = p.parseVar()
    }
    
    return imp
}
```

### 3.4 规则解析

**parseRule实现**:

```go
// parseRule: 解析规则
func (p *parser) parseRule() *Rule {
    rule := &Rule{
        Location: p.tokPos.Location,
    }
    
    // 1. 解析 default 关键字
    if p.match(DEFAULT) {
        rule.Default = true
    }
    
    // 2. 解析规则头部
    rule.Head = p.parseRuleHead()
    
    // 3. 解析规则体（可选）
    if p.tok == IF || p.tok == LBRACE {
        rule.Body = p.parseRuleBody()
    }
    
    return rule
}

// parseRuleHead: 解析规则头部
func (p *parser) parseRuleHead() *Head {
    head := &Head{
        Location: p.tokPos.Location,
    }
    
    // 规则名称（可能是引用）
    head.Name = p.parseRef()
    
    // 函数参数
    if p.match(LPAREN) {
        head.Args = p.parseTermList()
        p.expect(RPAREN)
    }
    
    // 赋值或统一
    if p.match(ASSIGN) {
        // head := value
        head.Value = p.parseTerm()
    } else if p.match(EQ) {
        // head = value (等式)
        head.Value = p.parseTerm()
        head.Assign = false
    }
    
    return head
}

// parseRuleBody: 解析规则体
func (p *parser) parseRuleBody() Body {
    var body Body
    
    // 可选的 if 关键字 (Rego v1.0)
    p.match(IF)
    
    // 解析表达式列表
    if p.match(LBRACE) {
        // 多行规则体: { expr1; expr2; ... }
        for p.tok != RBRACE && p.tok != EOF {
            expr := p.parseExpr()
            body = append(body, expr)
            
            // 表达式分隔符（分号或换行）
            if !p.match(SEMICOLON) && p.tok != RBRACE {
                p.match(WHITESPACE)
            }
        }
        p.expect(RBRACE)
    } else {
        // 单行规则体: expr1; expr2
        for {
            expr := p.parseExpr()
            body = append(body, expr)
            
            if !p.match(SEMICOLON) {
                break
            }
        }
    }
    
    return body
}
```

### 3.5 表达式解析

**parseExpr实现**:

```go
// parseExpr: 解析表达式
func (p *parser) parseExpr() *Expr {
    return p.parseInfixExpr(0)
}

// parseInfixExpr: 解析中缀表达式（运算符优先级）
func (p *parser) parseInfixExpr(minPrec int) *Expr {
    // 解析左侧
    left := p.parseUnaryExpr()
    
    // 循环处理中缀运算符
    for {
        op := p.tok
        prec := precedence(op)
        
        if prec < minPrec {
            break
        }
        
        p.next()
        
        // 解析右侧（递归，处理结合性）
        right := p.parseInfixExpr(prec + 1)
        
        // 构建表达式
        left = &Expr{
            Location: left.Location,
            Terms: []*Term{
                &Term{Value: Ref{Var(op.String())}},  // 运算符
                &Term{Value: left},                     // 左操作数
                &Term{Value: right},                    // 右操作数
            },
        }
    }
    
    return left
}

// parseUnaryExpr: 解析一元表达式
func (p *parser) parseUnaryExpr() *Expr {
    switch p.tok {
    case NOT:
        // not expr
        p.next()
        expr := p.parseUnaryExpr()
        return &Expr{
            Negated: true,
            Terms:   expr.Terms,
        }
        
    case SOME:
        // some var in collection
        return p.parseSomeDecl()
        
    default:
        // 普通表达式
        return p.parsePrimaryExpr()
    }
}

// parsePrimaryExpr: 解析基本表达式
func (p *parser) parsePrimaryExpr() *Expr {
    // 表达式 = 项 | 项 运算符 项
    left := p.parseTerm()
    
    // 检查是否是二元运算符
    if isBinaryOp(p.tok) {
        return p.parseBinaryExpr(left)
    }
    
    // 单项表达式
    return &Expr{
        Location: left.Location,
        Terms:    []*Term{left},
    }
}
```

### 3.6 项（Term）解析

**parseTerm实现**:

```go
// parseTerm: 解析项
func (p *parser) parseTerm() *Term {
    term := &Term{
        Location: p.tokPos.Location,
    }
    
    switch p.tok {
    case IDENT:
        // 变量或引用
        term.Value = p.parseRefOrVar()
        
    case NULL:
        p.next()
        term.Value = Null{}
        
    case TRUE:
        p.next()
        term.Value = Boolean(true)
        
    case FALSE:
        p.next()
        term.Value = Boolean(false)
        
    case NUMBER:
        term.Value = p.parseNumber()
        
    case STRING, RAWSTRING:
        term.Value = p.parseString()
        
    case LBRACK:
        // 数组: [1, 2, 3] 或 推导式: [x | ...]
        term.Value = p.parseArrayOrComprehension()
        
    case LBRACE:
        // 对象: {a: 1} 或 集合: {1, 2} 或 推导式
        term.Value = p.parseObjectOrSetOrComprehension()
        
    case LPAREN:
        // 括号表达式
        p.next()
        inner := p.parseTerm()
        p.expect(RPAREN)
        return inner
        
    default:
        p.errorf("unexpected token in term: %s", p.tok)
        p.next()
    }
    
    return term
}

// parseRefOrVar: 解析引用或变量
func (p *parser) parseRefOrVar() Value {
    // 开始是标识符
    name := p.tokVal
    p.next()
    
    // 检查是否是引用（有.或[）
    if p.tok == DOT || p.tok == LBRACK {
        ref := Ref{Var(name)}
        return p.parseRefSuffix(ref)
    }
    
    // 单纯的变量
    return Var(name)
}

// parseRefSuffix: 解析引用后缀
func (p *parser) parseRefSuffix(ref Ref) Ref {
    for {
        switch p.tok {
        case DOT:
            // .field
            p.next()
            if p.tok != IDENT {
                p.errorf("expected identifier after .")
                return ref
            }
            ref = append(ref, StringTerm(p.tokVal))
            p.next()
            
        case LBRACK:
            // [index] 或 [key]
            p.next()
            index := p.parseTerm()
            ref = append(ref, index)
            p.expect(RBRACK)
            
        default:
            return ref
        }
    }
}
```

---

## 4. 运算符优先级

### 4.1 优先级表

**运算符优先级**（从低到高）:

```go
const (
    PREC_LOWEST      = iota
    PREC_ASSIGNMENT  // :=
    PREC_OR          // |
    PREC_AND         // &
    PREC_NOT         // not
    PREC_COMPARE     // ==, !=, <, <=, >, >=
    PREC_ADD         // +, -
    PREC_MUL         // *, /, %
    PREC_UNARY       // -x, not x
    PREC_CALL        // f(x)
    PREC_INDEX       // x[i], x.field
)

// precedence: 获取运算符优先级
func precedence(tok token) int {
    switch tok {
    case ASSIGN:
        return PREC_ASSIGNMENT
    case OR:
        return PREC_OR
    case AND:
        return PREC_AND
    case EQ, NEQ, LT, LTE, GT, GTE:
        return PREC_COMPARE
    case PLUS, MINUS:
        return PREC_ADD
    case MUL, DIV, MOD:
        return PREC_MUL
    }
    return PREC_LOWEST
}
```

### 4.2 表达式解析算法

**Precedence Climbing算法**:

```go
// parseInfixExpr: 优先级爬升算法
func (p *parser) parseInfixExpr(minPrec int) *Expr {
    // 解析左操作数
    left := p.parsePrimary()
    
    for {
        // 查看当前运算符
        op := p.tok
        prec := precedence(op)
        
        // 优先级不够，返回
        if prec < minPrec {
            break
        }
        
        p.next()  // 消费运算符
        
        // 递归解析右操作数
        // prec + 1 保证左结合
        right := p.parseInfixExpr(prec + 1)
        
        // 构建新的表达式
        left = makeBinaryExpr(op, left, right)
    }
    
    return left
}
```

**示例**:

```text
输入: a + b * c - d

解析过程:
1. parseInfixExpr(0)
   - left = a
   - 看到 +, prec=2 >= 0, 消费
   - right = parseInfixExpr(3)
     - left = b
     - 看到 *, prec=3 >= 3, 消费
     - right = parseInfixExpr(4)
       - left = c
       - 看到 -, prec=2 < 4, 返回 c
     - 构建 b * c
     - 看到 -, prec=2 < 3, 返回 b*c
   - 构建 a + (b*c)
   - 看到 -, prec=2 >= 0, 消费
   - right = parseInfixExpr(3)
     - left = d
     - 看到 EOF, 返回 d
   - 构建 (a + (b*c)) - d

结果AST:
      -
     / \
    +   d
   / \
  a   *
     / \
    b   c
```

### 4.3 Pratt Parsing

OPA的表达式解析器借鉴了**Pratt Parsing**思想：

```go
// Pratt Parser核心思想
type prefixParseFn func() *Expr
type infixParseFn func(*Expr) *Expr

type parser struct {
    prefixFns map[token]prefixParseFn
    infixFns  map[token]infixParseFn
}

// 注册前缀解析函数
func (p *parser) registerPrefix(tok token, fn prefixParseFn) {
    p.prefixFns[tok] = fn
}

// 注册中缀解析函数
func (p *parser) registerInfix(tok token, fn infixParseFn) {
    p.infixFns[tok] = fn
}
```

---

## 5. 错误处理

### 5.1 错误类型

**错误定义**:

```go
// Error: 解析错误
type Error struct {
    Location *Location  // 错误位置
    Message  string     // 错误信息
    Code     string     // 错误代码
}

// Errors: 错误列表
type Errors []Error

// 常见错误代码
const (
    ParseErr        = "rego_parse_error"
    CompileErr      = "rego_compile_error"
    TypeError       = "rego_type_error"
    UnsafeVarErr    = "rego_unsafe_var_error"
)

// errorf: 创建错误
func (p *parser) errorf(format string, args ...interface{}) {
    err := Error{
        Location: p.tokPos.Location,
        Message:  fmt.Sprintf(format, args...),
        Code:     ParseErr,
    }
    p.errors = append(p.errors, err)
}
```

### 5.2 错误恢复策略

**Panic Mode恢复**:

```go
// recover: 错误恢复
func (p *parser) recover() {
    p.panicMode = true
    
    // 跳过token直到找到同步点
    for !p.atSyncPoint() && p.tok != EOF {
        p.next()
    }
    
    p.panicMode = false
}

// atSyncPoint: 是否在同步点
func (p *parser) atSyncPoint() bool {
    switch p.tok {
    case PACKAGE, IMPORT, DEFAULT:
        return true
    case RBRACE:
        return !p.inRule
    }
    return false
}

// parseRule with recovery
func (p *parser) parseRule() *Rule {
    defer func() {
        if r := recover(); r != nil {
            p.recover()
        }
    }()
    
    rule := &Rule{}
    
    // ... 解析逻辑 ...
    
    return rule
}
```

### 5.3 错误报告

**友好的错误信息**:

```go
// formatError: 格式化错误信息
func formatError(err Error, source []byte) string {
    var buf bytes.Buffer
    
    // 错误头部
    fmt.Fprintf(&buf, "%s:%d:%d: %s\n",
        err.Location.File,
        err.Location.Row,
        err.Location.Col,
        err.Message)
    
    // 显示源代码行
    line := getLine(source, err.Location.Row)
    fmt.Fprintf(&buf, "%s\n", line)
    
    // 显示错误位置标记
    fmt.Fprintf(&buf, "%s^\n", strings.Repeat(" ", err.Location.Col-1))
    
    return buf.String()
}

// 示例输出:
// policy.rego:5:12: expected ), got IDENT
// allow if user = "admin"
//            ^
```

### 5.4 位置追踪

**位置信息**:

```go
// Location: 源码位置
type Location struct {
    File   string  // 文件名
    Row    int     // 行号（从1开始）
    Col    int     // 列号（从1开始）
    Offset int     // 字节偏移
}

// Pos: 位置信息
type Pos struct {
    Location *Location
}

// position: 获取当前位置
func (s *scanner) position() Pos {
    return Pos{
        Location: &Location{
            File:   s.filename,
            Row:    s.lineNum,
            Col:    s.linePos,
            Offset: s.offset,
        },
    }
}

// 在AST节点中保存位置
type Rule struct {
    Location *Location  // 规则定义位置
    Head     *Head
    Body     Body
}
```

---

## 6. 特殊语法处理

### 6.1 集合推导式

**语法**: `{ x | x = arr[_]; x > 5 }`

**解析实现**:

```go
// parseSetComprehension: 解析集合推导式
func (p *parser) parseSetComprehension() *SetComprehension {
    // { term | body }
    p.expect(LBRACE)
    
    comp := &SetComprehension{
        Location: p.tokPos.Location,
    }
    
    // 解析项（结果表达式）
    comp.Term = p.parseTerm()
    
    // 期望 |
    p.expect(OR)
    
    // 解析推导体
    comp.Body = p.parseBody()
    
    p.expect(RBRACE)
    
    return comp
}

// AST表示:
// SetComprehension {
//     Term: Var("x"),
//     Body: [
//         Expr(x = arr[_]),
//         Expr(x > 5),
//     ]
// }
```

### 6.2 对象推导式

**语法**: `{ k: v | k = keys[_]; v = values[k] }`

**解析实现**:

```go
// parseObjectComprehension: 解析对象推导式
func (p *parser) parseObjectComprehension() *ObjectComprehension {
    // { key: value | body }
    p.expect(LBRACE)
    
    comp := &ObjectComprehension{
        Location: p.tokPos.Location,
    }
    
    // 解析键
    comp.Key = p.parseTerm()
    
    p.expect(COLON)
    
    // 解析值
    comp.Value = p.parseTerm()
    
    p.expect(OR)
    
    // 解析推导体
    comp.Body = p.parseBody()
    
    p.expect(RBRACE)
    
    return comp
}
```

### 6.3 数组推导式

**语法**: `[ x | x = arr[_]; x > 5 ]`

**解析实现**:

```go
// parseArrayComprehension: 解析数组推导式
func (p *parser) parseArrayComprehension() *ArrayComprehension {
    // [ term | body ]
    p.expect(LBRACK)
    
    comp := &ArrayComprehension{
        Location: p.tokPos.Location,
    }
    
    // 解析项
    comp.Term = p.parseTerm()
    
    p.expect(OR)
    
    // 解析推导体
    comp.Body = p.parseBody()
    
    p.expect(RBRACK)
    
    return comp
}
```

### 6.4 引用（Reference）

**语法**: `data.users[0].name`

**解析实现**:

```go
// parseRef: 解析引用
func (p *parser) parseRef() Ref {
    ref := Ref{}
    
    // 第一部分：必须是标识符
    if p.tok != IDENT {
        p.errorf("expected identifier")
        return ref
    }
    
    ref = append(ref, VarTerm(p.tokVal))
    p.next()
    
    // 后续部分：.field 或 [index]
    for {
        switch p.tok {
        case DOT:
            // .field
            p.next()
            if p.tok != IDENT {
                p.errorf("expected identifier after .")
                return ref
            }
            ref = append(ref, StringTerm(p.tokVal))
            p.next()
            
        case LBRACK:
            // [index]
            p.next()
            
            // 特殊处理 [_] (通配符)
            if p.tok == IDENT && p.tokVal == "_" {
                ref = append(ref, VarTerm("$_"))
                p.next()
            } else {
                index := p.parseTerm()
                ref = append(ref, index)
            }
            
            p.expect(RBRACK)
            
        default:
            return ref
        }
    }
}

// 示例AST:
// data.users[0].name →
// Ref[
//     Var("data"),
//     String("users"),
//     Number(0),
//     String("name"),
// ]
```

### 6.5 函数调用

**语法**: `count(arr)`, `startswith(str, prefix)`

**解析实现**:

```go
// parseFunctionCall: 解析函数调用
func (p *parser) parseFunctionCall(name *Term) *Term {
    // name(arg1, arg2, ...)
    
    call := &Call{
        Location: name.Location,
        Func:     name,
        Args:     []*Term{},
    }
    
    p.expect(LPAREN)
    
    // 解析参数列表
    if p.tok != RPAREN {
        for {
            arg := p.parseTerm()
            call.Args = append(call.Args, arg)
            
            if !p.match(COMMA) {
                break
            }
        }
    }
    
    p.expect(RPAREN)
    
    return &Term{
        Location: name.Location,
        Value:    call,
    }
}

// 在parseTerm中检测函数调用:
func (p *parser) parseTerm() *Term {
    term := p.parseBasicTerm()
    
    // 检查是否是函数调用
    if p.tok == LPAREN {
        return p.parseFunctionCall(term)
    }
    
    return term
}
```

---

## 7. 源码实现分析

### 7.1 Scanner核心代码

**实际OPA Scanner代码片段**:

```go
// 来自: ast/parser.go

// scanner实现
type scanner struct {
    reader *bytes.Reader
    buffer []byte
    
    // 位置
    offset int
    row    int
    col    int
    
    // 当前字符
    ch rune
}

// scan方法（简化版）
func (s *scanner) scan() (token, string, Pos) {
    s.skipWhitespace()
    
    pos := s.pos()
    ch := s.ch
    
    switch {
    case isLetter(ch):
        lit := s.scanIdentifier()
        tok := lookupKeyword(lit)
        return tok, lit, pos
        
    case isDigit(ch) || ch == '-' && isDigit(s.peek()):
        return s.scanNumber()
        
    case ch == '"':
        return s.scanString()
        
    case ch == '#':
        s.scanComment()
        return s.scan()  // 递归跳过注释
    }
    
    // 运算符和标点
    s.next()
    
    switch ch {
    case ':':
        if s.ch == '=' {
            s.next()
            return ASSIGN, ":=", pos
        }
        return COLON, ":", pos
        
    case '=':
        if s.ch == '=' {
            s.next()
            return EQ, "==", pos
        }
        return ILLEGAL, "=", pos
        
    // ... 其他运算符 ...
    }
    
    return ILLEGAL, string(ch), pos
}
```

### 7.2 Parser核心代码

**实际OPA Parser代码片段**:

```go
// 来自: ast/parser.go

// parseModule实现
func (p *parser) parseModule(filename string) (*Module, error) {
    module := &Module{}
    
    // Package声明
    if p.tok == PACKAGE {
        module.Package = p.parsePackage()
    }
    
    // Import语句
    for p.tok == IMPORT {
        module.Imports = append(module.Imports, p.parseImport())
    }
    
    // 规则
    for p.tok != EOF {
        if stmt := p.parseStmt(); stmt != nil {
            module.Rules = append(module.Rules, stmt)
        }
    }
    
    return module, nil
}

// parseStmt: 解析语句（规则或注释）
func (p *parser) parseStmt() interface{} {
    switch p.tok {
    case DEFAULT:
        return p.parseRule(true)
    case COMMENT:
        c := p.parseComment()
        p.next()
        return c
    default:
        return p.parseRule(false)
    }
}

// parseRule的真实实现（简化）
func (p *parser) parseRule(isDefault bool) *Rule {
    rule := &Rule{
        Default: isDefault,
    }
    
    // 规则头部
    rule.Head = p.parseHead()
    
    // 规则体（可选）
    if p.tok == IF || p.tok == LBRACE {
        p.match(IF)  // Rego v1.0语法
        rule.Body = p.parseBody()
    } else {
        rule.Body = NewBody()
    }
    
    return rule
}
```

### 7.3 性能优化

**OPA解析器的性能优化**:

1. **零拷贝字符串**:

    ```go
    // 使用byte slice视图，避免内存拷贝
    func (s *scanner) scanIdentifier() string {
        start := s.offset
        s.scanIdentChars()
        // 直接返回底层buffer的字符串视图
        return string(s.buffer[start:s.offset])
    }
    ```

2. **预分配缓冲区**:

    ```go
    // 预分配常见大小的slice
    func parseBody() Body {
        body := make(Body, 0, 8)  // 大多数规则体 < 8个表达式
        // ...
        return body
    }
    ```

3. **关键字查找表**:

    ```go
    // 使用map而不是if-else链
    var keywords = map[string]token{
        "package": PACKAGE,
        "import":  IMPORT,
        // ...
    }
    ```

4. **惰性AST构建**:

    ```go
    // 只在需要时构建完整AST
    type LazyRule struct {
        raw    []byte
        parsed *Rule
    }

    func (r *LazyRule) Parse() *Rule {
        if r.parsed == nil {
            r.parsed = parseRule(r.raw)
        }
        return r.parsed
    }
    ```

---

## 8. 实战示例

### 8.1 简单规则解析

**输入Rego**:

```rego
package example

allow if {
    input.method == "GET"
    input.user.role == "admin"
}
```

**解析过程**:

```go
// 1. 词法分析
tokens := [
    PACKAGE, IDENT("example"),
    IDENT("allow"), IF, LBRACE,
    IDENT("input"), DOT, IDENT("method"), EQ, EQ, STRING("GET"),
    IDENT("input"), DOT, IDENT("user"), DOT, IDENT("role"), EQ, EQ, STRING("admin"),
    RBRACE,
    EOF,
]

// 2. 语法分析
module := &Module{
    Package: &Package{
        Path: Ref{Var("example")},
    },
    Rules: []*Rule{
        {
            Head: &Head{
                Name: Ref{Var("allow")},
            },
            Body: Body{
                &Expr{
                    Terms: []*Term{
                        &Term{Value: Ref{Var("input"), String("method")}},
                        &Term{Value: String("GET")},
                    },
                    Operator: EQ,
                },
                &Expr{
                    Terms: []*Term{
                        &Term{Value: Ref{Var("input"), String("user"), String("role")}},
                        &Term{Value: String("admin")},
                    },
                    Operator: EQ,
                },
            },
        },
    },
}
```

### 8.2 复杂表达式解析

**输入Rego**:

```rego
result := (a + b) * c - d / e
```

**运算符优先级解析**:

```text
Token流: result ASSIGN LPAREN a PLUS b RPAREN MUL c MINUS d DIV e

解析过程:
1. result := ...
   - parseRule()
   - parseHead()
     - name = "result"
     - ASSIGN
     - value = parseExpr()

2. parseExpr() with minPrec=0
   - left = parsePrimary()
     - LPAREN
     - inner = (a + b)  # 递归
     - RPAREN
   
   - 看到 MUL, prec=3 >= 0
   - right = parseExpr(4)
     - left = c
     - 看到 MINUS, prec=2 < 4, 返回 c
   - 构建: (a+b) * c
   
   - 看到 MINUS, prec=2 >= 0
   - right = parseExpr(3)
     - left = d
     - 看到 DIV, prec=3 >= 3
     - right = parseExpr(4)
       - left = e
       - EOF
     - 构建: d / e
   - 构建: ((a+b)*c) - (d/e)

最终AST:
        ASSIGN
       /      \
    result    MINUS
              /     \
            MUL     DIV
           /   \   /   \
         PLUS  c  d     e
        /    \
       a      b
```

### 8.3 错误处理示例

**输入（有错误）**:

```rego
package example

allow if {
    input.method == GET"   # 缺少双引号
    input.user.role = "admin"  # 应该是 ==
}
```

**错误报告**:

```text
example.rego:4:21: expected STRING, got IDENT
    input.method == GET"
                    ^

example.rego:5:21: illegal token '='
    input.user.role = "admin"
                    ^

Parsed with 2 errors
```

---

## 9. 测试与验证

### 9.1 单元测试

**测试Scanner**:

```go
func TestScanner(t *testing.T) {
    tests := []struct {
        input    string
        expected []token
    }{
        {
            input: "package example",
            expected: []token{PACKAGE, IDENT, EOF},
        },
        {
            input: "x := 10",
            expected: []token{IDENT, ASSIGN, NUMBER, EOF},
        },
        {
            input: `"hello \"world\""`,
            expected: []token{STRING, EOF},
        },
    }
    
    for _, tt := range tests {
        s := newScanner([]byte(tt.input))
        
        for i, exp := range tt.expected {
            tok, _, _ := s.scan()
            if tok != exp {
                t.Errorf("test %d: expected %s, got %s", i, exp, tok)
            }
        }
    }
}
```

**测试Parser**:

```go
func TestParser(t *testing.T) {
    input := `
package example

allow if {
    input.method == "GET"
}
`
    
    module, err := ParseModule("test.rego", []byte(input))
    if err != nil {
        t.Fatal(err)
    }
    
    // 验证AST结构
    if module.Package.Path.String() != "data.example" {
        t.Errorf("wrong package path")
    }
    
    if len(module.Rules) != 1 {
        t.Errorf("expected 1 rule, got %d", len(module.Rules))
    }
    
    rule := module.Rules[0]
    if rule.Head.Name.String() != "allow" {
        t.Errorf("wrong rule name")
    }
    
    if len(rule.Body) != 1 {
        t.Errorf("expected 1 expression in body")
    }
}
```

### 9.2 性能测试

**Benchmark**:

```go
func BenchmarkParser(b *testing.B) {
    input := []byte(`
package example

allow if {
    input.method == "GET"
    input.path == "/api"
    input.user.role == "admin"
}
`)
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        _, err := ParseModule("test.rego", input)
        if err != nil {
            b.Fatal(err)
        }
    }
}

// 结果示例:
// BenchmarkParser-8    50000    25000 ns/op    8192 B/op    150 allocs/op
```

### 9.3 Fuzzing测试

**Fuzz测试**:

```go
func FuzzParser(f *testing.F) {
    // 添加种子语料
    f.Add([]byte("package example"))
    f.Add([]byte("allow if { true }"))
    
    f.Fuzz(func(t *testing.T, data []byte) {
        // 解析不应该panic
        _, _ = ParseModule("fuzz.rego", data)
    })
}

// 运行: go test -fuzz=FuzzParser
```

---

## 附录

### A. Token完整列表

**所有Token类型**:

```go
const (
    // 特殊
    ILLEGAL token = iota
    EOF
    COMMENT
    
    // 字面量
    IDENT
    NUMBER
    STRING
    RAWSTRING
    
    // 关键字
    AS
    DEFAULT
    ELSE
    FALSE
    IF
    IMPORT
    IN
    NOT
    NULL
    PACKAGE
    SOME
    TRUE
    WITH
    EVERY
    CONTAINS
    
    // 运算符
    ASSIGN     // :=
    EQ         // ==
    NEQ        // !=
    LT         // <
    LTE        // <=
    GT         // >
    GTE        // >=
    PLUS       // +
    MINUS      // -
    MUL        // *
    DIV        // /
    MOD        // %
    AND        // &
    OR         // |
    
    // 分隔符
    LPAREN     // (
    RPAREN     // )
    LBRACK     // [
    RBRACK     // ]
    LBRACE     // {
    RBRACE     // }
    COMMA      // ,
    DOT        // .
    SEMICOLON  // ;
    COLON      // :
    PIPE       // |
)
```

### B. 语法规则BNF

**Rego语法BNF**:

```bnf
Module        ::= Package Imports* Rules*
Package       ::= "package" Ref
Import        ::= "import" Ref ("as" Var)?

Rule          ::= "default"? RuleHead RuleBody?
RuleHead      ::= Ref ("(" TermList? ")")? (":=" | "=")? Term
RuleBody      ::= "if"? Body

Body          ::= "{" Expr* "}" | Expr
Expr          ::= Term
                | Expr InfixOp Expr
                | "not" Expr
                | "some" VarList "in" Term

Term          ::= Ref
                | Var
                | Scalar
                | Array
                | Object
                | Set
                | Comprehension
                | "(" Term ")"

Ref           ::= Var (RefArg)*
RefArg        ::= "." Var | "[" Term "]"

Array         ::= "[" TermList? "]"
Object        ::= "{" ObjectItemList? "}"
Set           ::= "{" TermList "}"

Comprehension ::= ArrayComp | ObjectComp | SetComp
ArrayComp     ::= "[" Term "|" Body "]"
ObjectComp    ::= "{" Term ":" Term "|" Body "}"
SetComp       ::= "{" Term "|" Body "}"

Scalar        ::= String | Number | Boolean | Null
Boolean       ::= "true" | "false"
Null          ::= "null"

InfixOp       ::= "==" | "!=" | "<" | "<=" | ">" | ">="
                | "+" | "-" | "*" | "/" | "%"
                | "&" | "|"
```

### C. 调试工具

**AST可视化工具**:

```go
// printAST: 打印AST结构
func printAST(node interface{}, indent int) {
    prefix := strings.Repeat("  ", indent)
    
    switch n := node.(type) {
    case *Module:
        fmt.Printf("%sModule\n", prefix)
        printAST(n.Package, indent+1)
        for _, imp := range n.Imports {
            printAST(imp, indent+1)
        }
        for _, rule := range n.Rules {
            printAST(rule, indent+1)
        }
        
    case *Rule:
        fmt.Printf("%sRule\n", prefix)
        printAST(n.Head, indent+1)
        printAST(n.Body, indent+1)
        
    case *Head:
        fmt.Printf("%sHead: %s\n", prefix, n.Name)
        
    case Body:
        fmt.Printf("%sBody (%d exprs)\n", prefix, len(n))
        for _, expr := range n {
            printAST(expr, indent+1)
        }
        
    // ... 其他节点类型 ...
    }
}

// 使用:
module, _ := ParseModule("example.rego", source)
printAST(module, 0)
```

**Token流查看器**:

```go
// showTokens: 显示token流
func showTokens(source []byte) {
    s := newScanner(source)
    
    fmt.Println("TOKEN          VALUE          POS")
    fmt.Println("─────────────────────────────────────")
    
    for {
        tok, val, pos := s.scan()
        
        fmt.Printf("%-12s   %-12s   %d:%d\n",
            tok.String(),
            val,
            pos.Row,
            pos.Col)
        
        if tok == EOF {
            break
        }
    }
}
```

---

**文档版本**: v1.0  
**最后更新**: 2025年10月23日  
**维护者**: OPA技术文档项目  
**反馈**: 欢迎通过GitHub Issues提供建议

**相关阅读**:

- [OPA架构总览与代码结构](10.1-OPA架构总览与代码结构.md) - 整体架构
- [AST构建与转换](10.3-AST构建与转换.md) - AST操作详解
- [编译器实现详解](10.4-编译器实现详解.md) - 编译优化
- [Rego语法规范](../02-语言模型/02.1-Rego语法规范.md) - 语法参考
