# è¯æ³•åˆ†æä¸è¯­æ³•è§£æï¼ˆLexical Analysis & Parsingï¼‰

> **é€‚ç”¨ç‰ˆæœ¬**: OPA v0.42+ (Rego v1è§£æå™¨) | æ¨è v0.68+  
> **å®ç°è¯­è¨€**: Go 1.20+  
> **æºç **: `github.com/open-policy-agent/opa/ast`  
> **æœ€åéªŒè¯**: 2025-10-21  
> **æ–‡æ¡£çŠ¶æ€**: âœ… å·²éªŒè¯

---

## ğŸ“– é˜…è¯»æç¤º

> **æœ¬æ–‡æ¡£é€‚åˆ**:
>
> - âœ… å¯¹ç¼–è¯‘åŸç†æœ‰åŸºç¡€çš„å¼€å‘è€…
> - âœ… éœ€è¦ç†è§£Regoè¯­æ³•å®ç°ç»†èŠ‚
> - âœ… å¸Œæœ›ä¸ºOPAé¡¹ç›®è´¡çŒ®ä»£ç 
> - âš ï¸ ç†è®ºæ€§å¼ºï¼Œåˆå­¦è€…å»ºè®®å…ˆå­¦ä¹ [Regoè¯­æ³•](../02-è¯­è¨€æ¨¡å‹/02.1-Regoè¯­æ³•è§„èŒƒ.md)
>
> **å®è·µä»·å€¼**:
>
> - ç†è§£è§£æè¿‡ç¨‹æœ‰åŠ©äºè°ƒè¯•è¯­æ³•é”™è¯¯
> - ASTç»“æ„çŸ¥è¯†å¯¹é«˜çº§åº”ç”¨æœ‰å¸®åŠ©
>
> ç›¸å…³: [ASTä¸IR](03.2-ASTä¸IR.md) | [ç¼–è¯‘å™¨è®¾è®¡](03.3-ç¼–è¯‘å™¨è®¾è®¡.md)

---

## ç›®å½•

- [è¯æ³•åˆ†æä¸è¯­æ³•è§£æï¼ˆLexical Analysis \& Parsingï¼‰](#è¯æ³•åˆ†æä¸è¯­æ³•è§£ælexical-analysis--parsing)
  - [ğŸ“– é˜…è¯»æç¤º](#-é˜…è¯»æç¤º)
  - [ç›®å½•](#ç›®å½•)
  - [1. ç¼–è¯‘æµæ°´çº¿æ¦‚è¿°](#1-ç¼–è¯‘æµæ°´çº¿æ¦‚è¿°)
    - [1.1 å®Œæ•´æµç¨‹](#11-å®Œæ•´æµç¨‹)
    - [1.2 å„é˜¶æ®µèŒè´£](#12-å„é˜¶æ®µèŒè´£)
  - [2. è¯æ³•åˆ†æ](#2-è¯æ³•åˆ†æ)
    - [2.1 è¯æ³•å•å…ƒï¼ˆTokensï¼‰](#21-è¯æ³•å•å…ƒtokens)
    - [2.2 è¯æ³•åˆ†æå™¨å®ç°](#22-è¯æ³•åˆ†æå™¨å®ç°)
  - [3. è¯­æ³•è§£æ](#3-è¯­æ³•è§£æ)
    - [3.1 è§£æå™¨ç»“æ„](#31-è§£æå™¨ç»“æ„)
    - [3.2 é€’å½’ä¸‹é™è§£æ](#32-é€’å½’ä¸‹é™è§£æ)
  - [4. ASTæ„å»º](#4-astæ„å»º)
    - [4.1 ASTèŠ‚ç‚¹ç±»å‹](#41-astèŠ‚ç‚¹ç±»å‹)
    - [4.2 ASTç¤ºä¾‹](#42-astç¤ºä¾‹)
  - [5. é”™è¯¯å¤„ç†](#5-é”™è¯¯å¤„ç†)
    - [5.1 é”™è¯¯ç±»å‹](#51-é”™è¯¯ç±»å‹)
    - [5.2 é”™è¯¯æŠ¥å‘Š](#52-é”™è¯¯æŠ¥å‘Š)
  - [6. å®è·µç¤ºä¾‹](#6-å®è·µç¤ºä¾‹)
    - [6.1 ä½¿ç”¨Goè§£æRego](#61-ä½¿ç”¨goè§£ærego)
    - [6.2 è‡ªå®šä¹‰ASTéå†](#62-è‡ªå®šä¹‰astéå†)
  - [é™„å½•: è§£æå™¨æ€§èƒ½](#é™„å½•-è§£æå™¨æ€§èƒ½)

---

## 1. ç¼–è¯‘æµæ°´çº¿æ¦‚è¿°

### 1.1 å®Œæ•´æµç¨‹

```text
Regoæºç  (Source Code)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Lexer (è¯æ³•åˆ†æå™¨)  â”‚  â†’ Tokenæµ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parser (è¯­æ³•è§£æå™¨) â”‚  â†’ AST (æŠ½è±¡è¯­æ³•æ ‘)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compiler (ç¼–è¯‘å™¨)   â”‚  â†’ IR (ä¸­é—´è¡¨ç¤º)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluator (æ±‚å€¼å™¨)  â”‚  â†’ ç»“æœ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å„é˜¶æ®µèŒè´£

| é˜¶æ®µ | è¾“å…¥ | è¾“å‡º | èŒè´£ |
|------|------|------|------|
| **Lexer** | å­—ç¬¦æµ | Tokenæµ | è¯†åˆ«å…³é”®å­—ã€æ ‡è¯†ç¬¦ã€å­—é¢é‡ |
| **Parser** | Tokenæµ | AST | æ„å»ºè¯­æ³•æ ‘ |
| **Compiler** | AST | IR | ç±»å‹æ£€æŸ¥ã€ä¼˜åŒ–ã€ç´¢å¼•æ„å»º |
| **Evaluator** | IR + Input | Result | æ‰§è¡Œç­–ç•¥æ±‚å€¼ |

---

## 2. è¯æ³•åˆ†æ

### 2.1 è¯æ³•å•å…ƒï¼ˆTokensï¼‰

**Tokenç±»å‹å®šä¹‰**:

```go
// ä½ç½®: ast/tokens.go
type Token struct {
    Type     TokenType  // Tokenç±»å‹
    Value    string     // å­—é¢å€¼
    Location *Location  // æºç ä½ç½®
}

type TokenType int

const (
    // å…³é”®å­—
    Package TokenType = iota
    Import
    As
    Default
    Not
    Some
    If
    In
    Every
    
    // å­—é¢é‡
    Ident      // æ ‡è¯†ç¬¦: allow, user
    String     // å­—ç¬¦ä¸²: "hello"
    Number     // æ•°å€¼: 42, 3.14
    Bool       // å¸ƒå°”: true, false
    Null       // ç©ºå€¼: null
    
    // è¿ç®—ç¬¦
    Assign     // :=
    Eq         // ==
    NEq        // !=
    LT         // <
    GT         // >
    LTE        // <=
    GTE        // >=
    Plus       // +
    Minus      // -
    Mult       // *
    Div        // /
    Mod        // %
    And        // &
    Or         // |
    Unify      // =
    
    // åˆ†éš”ç¬¦
    LParen     // (
    RParen     // )
    LBrace     // {
    RBrace     // }
    LBrack     // [
    RBrack     // ]
    Comma      // ,
    Dot        // .
    Colon      // :
    Semicolon  // ;
    Pipe       // |
    
    // ç‰¹æ®Š
    EOF        // æ–‡ä»¶ç»“æŸ
    Comment    // æ³¨é‡Š
    Whitespace // ç©ºç™½
)
```

---

### 2.2 è¯æ³•åˆ†æå™¨å®ç°

**æ ¸å¿ƒç»“æ„**:

```go
// ä½ç½®: ast/parser.go
type Lexer struct {
    source string     // æºä»£ç 
    pos    int        // å½“å‰ä½ç½®
    ch     rune       // å½“å‰å­—ç¬¦
    line   int        // å½“å‰è¡Œå·
    col    int        // å½“å‰åˆ—å·
}

func NewLexer(source string) *Lexer {
    l := &Lexer{
        source: source,
        pos:    0,
        line:   1,
        col:    1,
    }
    l.readChar()  // è¯»å–ç¬¬ä¸€ä¸ªå­—ç¬¦
    return l
}
```

**æ ¸å¿ƒæ–¹æ³•**:

```go
// è¯»å–ä¸‹ä¸€ä¸ªå­—ç¬¦
func (l *Lexer) readChar() {
    if l.pos >= len(l.source) {
        l.ch = 0  // EOF
    } else {
        l.ch = rune(l.source[l.pos])
        l.pos++
        l.col++
        if l.ch == '\n' {
            l.line++
            l.col = 1
        }
    }
}

// è·³è¿‡ç©ºç™½å­—ç¬¦
func (l *Lexer) skipWhitespace() {
    for l.ch == ' ' || l.ch == '\t' || l.ch == '\n' || l.ch == '\r' {
        l.readChar()
    }
}

// è¯»å–æ ‡è¯†ç¬¦
func (l *Lexer) readIdentifier() string {
    start := l.pos - 1
    for isLetter(l.ch) || isDigit(l.ch) || l.ch == '_' {
        l.readChar()
    }
    return l.source[start : l.pos-1]
}

// è¯»å–æ•°å€¼
func (l *Lexer) readNumber() string {
    start := l.pos - 1
    for isDigit(l.ch) {
        l.readChar()
    }
    // æµ®ç‚¹æ•°
    if l.ch == '.' {
        l.readChar()
        for isDigit(l.ch) {
            l.readChar()
        }
    }
    // ç§‘å­¦è®¡æ•°æ³•
    if l.ch == 'e' || l.ch == 'E' {
        l.readChar()
        if l.ch == '+' || l.ch == '-' {
            l.readChar()
        }
        for isDigit(l.ch) {
            l.readChar()
        }
    }
    return l.source[start : l.pos-1]
}

// è¯»å–å­—ç¬¦ä¸²
func (l *Lexer) readString() (string, error) {
    var buf bytes.Buffer
    l.readChar()  // è·³è¿‡å¼€å¤´çš„ "
    
    for l.ch != '"' {
        if l.ch == 0 {
            return "", errors.New("unterminated string")
        }
        if l.ch == '\\' {
            l.readChar()
            // å¤„ç†è½¬ä¹‰åºåˆ—
            switch l.ch {
            case 'n':
                buf.WriteRune('\n')
            case 't':
                buf.WriteRune('\t')
            case '"':
                buf.WriteRune('"')
            case '\\':
                buf.WriteRune('\\')
            case 'u':
                // Unicodeè½¬ä¹‰: \u4e2d
                l.readChar()
                hex := l.source[l.pos-1 : l.pos+3]
                code, _ := strconv.ParseInt(hex, 16, 32)
                buf.WriteRune(rune(code))
                l.pos += 3
                l.readChar()
                continue
            default:
                return "", fmt.Errorf("invalid escape sequence: \\%c", l.ch)
            }
        } else {
            buf.WriteRune(l.ch)
        }
        l.readChar()
    }
    
    l.readChar()  // è·³è¿‡ç»“å°¾çš„ "
    return buf.String(), nil
}
```

**ä¸»æ‰«æå¾ªç¯**:

```go
func (l *Lexer) NextToken() Token {
    l.skipWhitespace()
    
    tok := Token{
        Location: &Location{
            File: l.filename,
            Row:  l.line,
            Col:  l.col,
        },
    }
    
    switch l.ch {
    case 0:
        tok.Type = EOF
    
    case '(':
        tok.Type = LParen
        tok.Value = "("
        l.readChar()
    
    case ')':
        tok.Type = RParen
        tok.Value = ")"
        l.readChar()
    
    case '{':
        tok.Type = LBrace
        tok.Value = "{"
        l.readChar()
    
    case '}':
        tok.Type = RBrace
        tok.Value = "}"
        l.readChar()
    
    case '=':
        if l.peekChar() == '=' {
            tok.Type = Eq
            tok.Value = "=="
            l.readChar()
            l.readChar()
        } else {
            tok.Type = Unify
            tok.Value = "="
            l.readChar()
        }
    
    case ':':
        if l.peekChar() == '=' {
            tok.Type = Assign
            tok.Value = ":="
            l.readChar()
            l.readChar()
        } else {
            tok.Type = Colon
            tok.Value = ":"
            l.readChar()
        }
    
    case '"':
        str, err := l.readString()
        if err != nil {
            tok.Type = Illegal
            tok.Value = err.Error()
        } else {
            tok.Type = String
            tok.Value = str
        }
    
    default:
        if isLetter(l.ch) {
            ident := l.readIdentifier()
            tok.Type = lookupKeyword(ident)
            tok.Value = ident
            return tok
        } else if isDigit(l.ch) {
            tok.Type = Number
            tok.Value = l.readNumber()
            return tok
        } else {
            tok.Type = Illegal
            tok.Value = string(l.ch)
            l.readChar()
        }
    }
    
    return tok
}
```

**å…³é”®å­—è¯†åˆ«**:

```go
var keywords = map[string]TokenType{
    "package":  Package,
    "import":   Import,
    "as":       As,
    "default":  Default,
    "not":      Not,
    "some":     Some,
    "if":       If,
    "in":       In,
    "every":    Every,
    "true":     Bool,
    "false":    Bool,
    "null":     Null,
}

func lookupKeyword(ident string) TokenType {
    if tok, ok := keywords[ident]; ok {
        return tok
    }
    return Ident
}
```

---

## 3. è¯­æ³•è§£æ

### 3.1 è§£æå™¨ç»“æ„

**æ ¸å¿ƒç»“æ„**:

```go
type Parser struct {
    lexer   *Lexer
    current Token      // å½“å‰token
    peek    Token      // ä¸‹ä¸€ä¸ªtoken
    errors  []error    // é”™è¯¯åˆ—è¡¨
}

func NewParser(source string) *Parser {
    p := &Parser{
        lexer: NewLexer(source),
    }
    // è¯»å–ä¸¤ä¸ªtokenåˆå§‹åŒ–currentå’Œpeek
    p.nextToken()
    p.nextToken()
    return p
}

func (p *Parser) nextToken() {
    p.current = p.peek
    p.peek = p.lexer.NextToken()
}
```

---

### 3.2 é€’å½’ä¸‹é™è§£æ

**è§£ææ¨¡å—**:

```go
func (p *Parser) ParseModule() (*Module, error) {
    module := &Module{}
    
    // è§£æpackageå£°æ˜
    if p.current.Type != Package {
        return nil, p.error("expected 'package'")
    }
    module.Package = p.parsePackage()
    
    // è§£æimportè¯­å¥
    for p.current.Type == Import {
        module.Imports = append(module.Imports, p.parseImport())
    }
    
    // è§£æè§„åˆ™
    for p.current.Type != EOF {
        rule, err := p.parseRule()
        if err != nil {
            return nil, err
        }
        module.Rules = append(module.Rules, rule)
    }
    
    return module, nil
}
```

**è§£æè§„åˆ™**:

```go
func (p *Parser) parseRule() (*Rule, error) {
    rule := &Rule{
        Location: p.current.Location,
    }
    
    // è§£æè§„åˆ™å¤´
    rule.Head = p.parseHead()
    
    // å¯é€‰: if å…³é”®å­—
    if p.current.Type == If {
        p.nextToken()
    }
    
    // è§£æè§„åˆ™ä½“
    if p.current.Type == LBrace {
        rule.Body = p.parseBody()
    }
    
    return rule, nil
}
```

**è§£æè§„åˆ™å¤´**:

```go
func (p *Parser) parseHead() *Head {
    head := &Head{}
    
    // è§„åˆ™å
    head.Name = p.parseRef()
    
    // å¯é€‰: [key]
    if p.current.Type == LBrack {
        p.nextToken()
        head.Key = p.parseTerm()
        p.expect(RBrack)
    }
    
    // å¯é€‰: := value
    if p.current.Type == Assign {
        p.nextToken()
        head.Value = p.parseTerm()
    }
    
    return head
}
```

**è§£æè§„åˆ™ä½“**:

```go
func (p *Parser) parseBody() Body {
    var body Body
    
    p.expect(LBrace)
    
    for p.current.Type != RBrace {
        expr := p.parseExpression()
        body = append(body, expr)
        
        // å¯é€‰çš„åˆ†å·æˆ–æ¢è¡Œ
        if p.current.Type == Semicolon {
            p.nextToken()
        }
    }
    
    p.expect(RBrace)
    
    return body
}
```

**è§£æè¡¨è¾¾å¼**:

```go
func (p *Parser) parseExpression() *Expr {
    expr := &Expr{
        Location: p.current.Location,
    }
    
    // å¦å®š
    if p.current.Type == Not {
        p.nextToken()
        expr.Negated = true
        expr.Terms = p.parseExpression().Terms
        return expr
    }
    
    // ç¬¬ä¸€ä¸ªé¡¹
    left := p.parseTerm()
    
    // äºŒå…ƒè¿ç®—ç¬¦
    if p.isBinaryOp(p.current.Type) {
        expr.Operator = p.current.Value
        p.nextToken()
        right := p.parseTerm()
        expr.Terms = []*Term{left, right}
    } else {
        // å•é¡¹è¡¨è¾¾å¼ï¼ˆå¦‚å‡½æ•°è°ƒç”¨ï¼‰
        expr.Terms = []*Term{left}
    }
    
    return expr
}
```

**è§£æé¡¹ï¼ˆTermï¼‰**:

```go
func (p *Parser) parseTerm() *Term {
    term := &Term{
        Location: p.current.Location,
    }
    
    switch p.current.Type {
    case Ident:
        // å¯èƒ½æ˜¯å˜é‡æˆ–å¼•ç”¨
        term.Value = p.parseRef()
    
    case String:
        term.Value = StringTerm(p.current.Value)
        p.nextToken()
    
    case Number:
        num, _ := strconv.ParseFloat(p.current.Value, 64)
        term.Value = NumberTerm(num)
        p.nextToken()
    
    case Bool:
        term.Value = BooleanTerm(p.current.Value == "true")
        p.nextToken()
    
    case Null:
        term.Value = NullTerm{}
        p.nextToken()
    
    case LBrack:
        // æ•°ç»„
        term.Value = p.parseArray()
    
    case LBrace:
        // å¯¹è±¡æˆ–é›†åˆ
        term.Value = p.parseObjectOrSet()
    
    case LParen:
        // åˆ†ç»„
        p.nextToken()
        term = p.parseTerm()
        p.expect(RParen)
    
    default:
        p.error(fmt.Sprintf("unexpected token: %v", p.current))
    }
    
    return term
}
```

**è§£ææ•°ç»„**:

```go
func (p *Parser) parseArray() *Array {
    arr := &Array{}
    
    p.expect(LBrack)
    
    // ç©ºæ•°ç»„
    if p.current.Type == RBrack {
        p.nextToken()
        return arr
    }
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯æ¨å¯¼å¼
    if p.isComprehension() {
        return p.parseArrayComprehension()
    }
    
    // æ™®é€šæ•°ç»„
    for {
        arr.Terms = append(arr.Terms, p.parseTerm())
        
        if p.current.Type != Comma {
            break
        }
        p.nextToken()
    }
    
    p.expect(RBrack)
    
    return arr
}
```

---

## 4. ASTæ„å»º

### 4.1 ASTèŠ‚ç‚¹ç±»å‹

**æ ¸å¿ƒèŠ‚ç‚¹**:

```go
// Module: æ¨¡å—ï¼ˆæ–‡ä»¶ï¼‰
type Module struct {
    Package *Package
    Imports []*Import
    Rules   []*Rule
}

// Rule: è§„åˆ™
type Rule struct {
    Head     *Head
    Body     Body
    Location *Location
}

// Head: è§„åˆ™å¤´
type Head struct {
    Name  *Term  // è§„åˆ™å
    Key   *Term  // éƒ¨åˆ†è§„åˆ™çš„é”®
    Value *Term  // è§„åˆ™å€¼
}

// Body: è§„åˆ™ä½“ï¼ˆè¡¨è¾¾å¼åˆ—è¡¨ï¼‰
type Body []*Expr

// Expr: è¡¨è¾¾å¼
type Expr struct {
    Negated  bool
    Terms    []*Term
    With     []*With
    Location *Location
}

// Term: é¡¹
type Term struct {
    Value    Value
    Location *Location
}

// Value: å€¼ï¼ˆæ¥å£ï¼Œå¤šç§å®ç°ï¼‰
type Value interface {
    String() string
}

// å…·ä½“å€¼ç±»å‹
type (
    Null      struct{}
    Boolean   bool
    Number    json.Number
    String    string
    Var       string
    Ref       []*Term
    Array     []*Term
    Object    [][2]*Term
    Set       []*Term
    Call      []*Term
)
```

---

### 4.2 ASTç¤ºä¾‹

**æºç **:

```rego
package example

import future.keywords.if

allow if {
    input.method == "GET"
    user := data.users[input.user_id]
    "read" in user.permissions
}
```

**ASTï¼ˆç®€åŒ–è¡¨ç¤ºï¼‰**:

```json
{
  "package": {
    "path": ["example"]
  },
  "imports": [
    {
      "path": ["future", "keywords", "if"]
    }
  ],
  "rules": [
    {
      "head": {
        "name": "allow"
      },
      "body": [
        {
          "terms": [
            {"ref": ["input", "method"]},
            {"string": "GET"}
          ],
          "operator": "=="
        },
        {
          "terms": [
            {"var": "user"},
            {
              "ref": [
                "data",
                "users",
                {"ref": ["input", "user_id"]}
              ]
            }
          ],
          "operator": ":="
        },
        {
          "terms": [
            {"string": "read"},
            {"ref": ["user", "permissions"]}
          ],
          "operator": "in"
        }
      ]
    }
  ]
}
```

---

## 5. é”™è¯¯å¤„ç†

### 5.1 é”™è¯¯ç±»å‹

```go
type ParserError struct {
    Message  string
    Location *Location
}

func (e *ParserError) Error() string {
    return fmt.Sprintf("%s:%d:%d: %s",
        e.Location.File,
        e.Location.Row,
        e.Location.Col,
        e.Message)
}
```

### 5.2 é”™è¯¯æŠ¥å‘Š

**ç¤ºä¾‹**:

```rego
package example

allow if {
    input.method = "GET"   # é”™è¯¯: åº”è¯¥ç”¨ ==
}
```

**é”™è¯¯è¾“å‡º**:

```text
policy.rego:4:18: rego_parse_error: unexpected token
    input.method = "GET"
                 ^
expected one of: ==, !=, <, >, <=, >=, in
```

---

## 6. å®è·µç¤ºä¾‹

### 6.1 ä½¿ç”¨Goè§£æRego

```go
package main

import (
    "fmt"
    "github.com/open-policy-agent/opa/ast"
)

func main() {
    source := `
        package example
        
        allow if {
            input.user == "admin"
        }
    `
    
    // è§£ææ¨¡å—
    module, err := ast.ParseModule("policy.rego", source)
    if err != nil {
        panic(err)
    }
    
    // è®¿é—®AST
    fmt.Println("Package:", module.Package)
    fmt.Println("Rules:", len(module.Rules))
    
    for _, rule := range module.Rules {
        fmt.Printf("Rule: %s\n", rule.Head.Name)
        fmt.Printf("Body expressions: %d\n", len(rule.Body))
    }
}
```

---

### 6.2 è‡ªå®šä¹‰ASTéå†

```go
// ASTè®¿é—®å™¨
type Visitor struct{}

func (v *Visitor) Visit(node interface{}) {
    switch n := node.(type) {
    case *ast.Rule:
        fmt.Printf("Visiting rule: %s\n", n.Head.Name)
    case *ast.Expr:
        fmt.Printf("Visiting expression\n")
    case *ast.Term:
        fmt.Printf("Visiting term: %v\n", n.Value)
    }
}

// éå†AST
func walkAST(module *ast.Module, visitor *Visitor) {
    for _, rule := range module.Rules {
        visitor.Visit(rule)
        for _, expr := range rule.Body {
            visitor.Visit(expr)
            for _, term := range expr.Terms {
                visitor.Visit(term)
            }
        }
    }
}
```

---

## é™„å½•: è§£æå™¨æ€§èƒ½

**åŸºå‡†æµ‹è¯•**:

```text
BenchmarkParseLarge-8    500    3.2 ms/op    2.1 MB/s
```

**ä¼˜åŒ–æŠ€å·§**:

1. ä½¿ç”¨å­—ç¬¦ä¸²æ± ï¼ˆString Interningï¼‰
2. ç¼“å­˜å¸¸ç”¨Token
3. é¢„åˆ†é…ASTèŠ‚ç‚¹åˆ‡ç‰‡
4. é¿å…ä¸å¿…è¦çš„å†…å­˜åˆ†é…

---

**ä¸‹ä¸€ç¯‡**: [03.2-ASTä¸IR](./03.2-ASTä¸IR.md)  
**ç›¸å…³**: [02.1-Regoè¯­æ³•è§„èŒƒ](../02-è¯­è¨€æ¨¡å‹/02.1-Regoè¯­æ³•è§„èŒƒ.md)
