# 词法分析与语法解析（Lexical Analysis & Parsing）

> **更新日期**: 2025年10月20日  
> **实现**: Go 1.22+ / OPA AST Package  
> **源码**: `github.com/open-policy-agent/opa/ast`

---

## 目录

- [词法分析与语法解析（Lexical Analysis \& Parsing）](#词法分析与语法解析lexical-analysis--parsing)
  - [目录](#目录)
  - [1. 编译流水线概述](#1-编译流水线概述)
    - [1.1 完整流程](#11-完整流程)
    - [1.2 各阶段职责](#12-各阶段职责)
  - [2. 词法分析](#2-词法分析)
    - [2.1 词法单元（Tokens）](#21-词法单元tokens)
    - [2.2 词法分析器实现](#22-词法分析器实现)
  - [3. 语法解析](#3-语法解析)
    - [3.1 解析器结构](#31-解析器结构)
    - [3.2 递归下降解析](#32-递归下降解析)
  - [4. AST构建](#4-ast构建)
    - [4.1 AST节点类型](#41-ast节点类型)
    - [4.2 AST示例](#42-ast示例)
  - [5. 错误处理](#5-错误处理)
    - [5.1 错误类型](#51-错误类型)
    - [5.2 错误报告](#52-错误报告)
  - [6. 实践示例](#6-实践示例)
    - [6.1 使用Go解析Rego](#61-使用go解析rego)
    - [6.2 自定义AST遍历](#62-自定义ast遍历)
  - [附录: 解析器性能](#附录-解析器性能)

---

## 1. 编译流水线概述

### 1.1 完整流程

```text
Rego源码 (Source Code)
    │
    ▼
┌─────────────────────┐
│  Lexer (词法分析器)  │  → Token流
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Parser (语法解析器) │  → AST (抽象语法树)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Compiler (编译器)   │  → IR (中间表示)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Evaluator (求值器)  │  → 结果
└─────────────────────┘
```

### 1.2 各阶段职责

| 阶段 | 输入 | 输出 | 职责 |
|------|------|------|------|
| **Lexer** | 字符流 | Token流 | 识别关键字、标识符、字面量 |
| **Parser** | Token流 | AST | 构建语法树 |
| **Compiler** | AST | IR | 类型检查、优化、索引构建 |
| **Evaluator** | IR + Input | Result | 执行策略求值 |

---

## 2. 词法分析

### 2.1 词法单元（Tokens）

**Token类型定义**:

```go
// 位置: ast/tokens.go
type Token struct {
    Type     TokenType  // Token类型
    Value    string     // 字面值
    Location *Location  // 源码位置
}

type TokenType int

const (
    // 关键字
    Package TokenType = iota
    Import
    As
    Default
    Not
    Some
    If
    In
    Every
    
    // 字面量
    Ident      // 标识符: allow, user
    String     // 字符串: "hello"
    Number     // 数值: 42, 3.14
    Bool       // 布尔: true, false
    Null       // 空值: null
    
    // 运算符
    Assign     // :=
    Eq         // ==
    NEq        // !=
    LT         // <
    GT         // >
    LTE        // <=
    GTE        // >=
    Plus       // +
    Minus      // -
    Mult       // *
    Div        // /
    Mod        // %
    And        // &
    Or         // |
    Unify      // =
    
    // 分隔符
    LParen     // (
    RParen     // )
    LBrace     // {
    RBrace     // }
    LBrack     // [
    RBrack     // ]
    Comma      // ,
    Dot        // .
    Colon      // :
    Semicolon  // ;
    Pipe       // |
    
    // 特殊
    EOF        // 文件结束
    Comment    // 注释
    Whitespace // 空白
)
```

---

### 2.2 词法分析器实现

**核心结构**:

```go
// 位置: ast/parser.go
type Lexer struct {
    source string     // 源代码
    pos    int        // 当前位置
    ch     rune       // 当前字符
    line   int        // 当前行号
    col    int        // 当前列号
}

func NewLexer(source string) *Lexer {
    l := &Lexer{
        source: source,
        pos:    0,
        line:   1,
        col:    1,
    }
    l.readChar()  // 读取第一个字符
    return l
}
```

**核心方法**:

```go
// 读取下一个字符
func (l *Lexer) readChar() {
    if l.pos >= len(l.source) {
        l.ch = 0  // EOF
    } else {
        l.ch = rune(l.source[l.pos])
        l.pos++
        l.col++
        if l.ch == '\n' {
            l.line++
            l.col = 1
        }
    }
}

// 跳过空白字符
func (l *Lexer) skipWhitespace() {
    for l.ch == ' ' || l.ch == '\t' || l.ch == '\n' || l.ch == '\r' {
        l.readChar()
    }
}

// 读取标识符
func (l *Lexer) readIdentifier() string {
    start := l.pos - 1
    for isLetter(l.ch) || isDigit(l.ch) || l.ch == '_' {
        l.readChar()
    }
    return l.source[start : l.pos-1]
}

// 读取数值
func (l *Lexer) readNumber() string {
    start := l.pos - 1
    for isDigit(l.ch) {
        l.readChar()
    }
    // 浮点数
    if l.ch == '.' {
        l.readChar()
        for isDigit(l.ch) {
            l.readChar()
        }
    }
    // 科学计数法
    if l.ch == 'e' || l.ch == 'E' {
        l.readChar()
        if l.ch == '+' || l.ch == '-' {
            l.readChar()
        }
        for isDigit(l.ch) {
            l.readChar()
        }
    }
    return l.source[start : l.pos-1]
}

// 读取字符串
func (l *Lexer) readString() (string, error) {
    var buf bytes.Buffer
    l.readChar()  // 跳过开头的 "
    
    for l.ch != '"' {
        if l.ch == 0 {
            return "", errors.New("unterminated string")
        }
        if l.ch == '\\' {
            l.readChar()
            // 处理转义序列
            switch l.ch {
            case 'n':
                buf.WriteRune('\n')
            case 't':
                buf.WriteRune('\t')
            case '"':
                buf.WriteRune('"')
            case '\\':
                buf.WriteRune('\\')
            case 'u':
                // Unicode转义: \u4e2d
                l.readChar()
                hex := l.source[l.pos-1 : l.pos+3]
                code, _ := strconv.ParseInt(hex, 16, 32)
                buf.WriteRune(rune(code))
                l.pos += 3
                l.readChar()
                continue
            default:
                return "", fmt.Errorf("invalid escape sequence: \\%c", l.ch)
            }
        } else {
            buf.WriteRune(l.ch)
        }
        l.readChar()
    }
    
    l.readChar()  // 跳过结尾的 "
    return buf.String(), nil
}
```

**主扫描循环**:

```go
func (l *Lexer) NextToken() Token {
    l.skipWhitespace()
    
    tok := Token{
        Location: &Location{
            File: l.filename,
            Row:  l.line,
            Col:  l.col,
        },
    }
    
    switch l.ch {
    case 0:
        tok.Type = EOF
    
    case '(':
        tok.Type = LParen
        tok.Value = "("
        l.readChar()
    
    case ')':
        tok.Type = RParen
        tok.Value = ")"
        l.readChar()
    
    case '{':
        tok.Type = LBrace
        tok.Value = "{"
        l.readChar()
    
    case '}':
        tok.Type = RBrace
        tok.Value = "}"
        l.readChar()
    
    case '=':
        if l.peekChar() == '=' {
            tok.Type = Eq
            tok.Value = "=="
            l.readChar()
            l.readChar()
        } else {
            tok.Type = Unify
            tok.Value = "="
            l.readChar()
        }
    
    case ':':
        if l.peekChar() == '=' {
            tok.Type = Assign
            tok.Value = ":="
            l.readChar()
            l.readChar()
        } else {
            tok.Type = Colon
            tok.Value = ":"
            l.readChar()
        }
    
    case '"':
        str, err := l.readString()
        if err != nil {
            tok.Type = Illegal
            tok.Value = err.Error()
        } else {
            tok.Type = String
            tok.Value = str
        }
    
    default:
        if isLetter(l.ch) {
            ident := l.readIdentifier()
            tok.Type = lookupKeyword(ident)
            tok.Value = ident
            return tok
        } else if isDigit(l.ch) {
            tok.Type = Number
            tok.Value = l.readNumber()
            return tok
        } else {
            tok.Type = Illegal
            tok.Value = string(l.ch)
            l.readChar()
        }
    }
    
    return tok
}
```

**关键字识别**:

```go
var keywords = map[string]TokenType{
    "package":  Package,
    "import":   Import,
    "as":       As,
    "default":  Default,
    "not":      Not,
    "some":     Some,
    "if":       If,
    "in":       In,
    "every":    Every,
    "true":     Bool,
    "false":    Bool,
    "null":     Null,
}

func lookupKeyword(ident string) TokenType {
    if tok, ok := keywords[ident]; ok {
        return tok
    }
    return Ident
}
```

---

## 3. 语法解析

### 3.1 解析器结构

**核心结构**:

```go
type Parser struct {
    lexer   *Lexer
    current Token      // 当前token
    peek    Token      // 下一个token
    errors  []error    // 错误列表
}

func NewParser(source string) *Parser {
    p := &Parser{
        lexer: NewLexer(source),
    }
    // 读取两个token初始化current和peek
    p.nextToken()
    p.nextToken()
    return p
}

func (p *Parser) nextToken() {
    p.current = p.peek
    p.peek = p.lexer.NextToken()
}
```

---

### 3.2 递归下降解析

**解析模块**:

```go
func (p *Parser) ParseModule() (*Module, error) {
    module := &Module{}
    
    // 解析package声明
    if p.current.Type != Package {
        return nil, p.error("expected 'package'")
    }
    module.Package = p.parsePackage()
    
    // 解析import语句
    for p.current.Type == Import {
        module.Imports = append(module.Imports, p.parseImport())
    }
    
    // 解析规则
    for p.current.Type != EOF {
        rule, err := p.parseRule()
        if err != nil {
            return nil, err
        }
        module.Rules = append(module.Rules, rule)
    }
    
    return module, nil
}
```

**解析规则**:

```go
func (p *Parser) parseRule() (*Rule, error) {
    rule := &Rule{
        Location: p.current.Location,
    }
    
    // 解析规则头
    rule.Head = p.parseHead()
    
    // 可选: if 关键字
    if p.current.Type == If {
        p.nextToken()
    }
    
    // 解析规则体
    if p.current.Type == LBrace {
        rule.Body = p.parseBody()
    }
    
    return rule, nil
}
```

**解析规则头**:

```go
func (p *Parser) parseHead() *Head {
    head := &Head{}
    
    // 规则名
    head.Name = p.parseRef()
    
    // 可选: [key]
    if p.current.Type == LBrack {
        p.nextToken()
        head.Key = p.parseTerm()
        p.expect(RBrack)
    }
    
    // 可选: := value
    if p.current.Type == Assign {
        p.nextToken()
        head.Value = p.parseTerm()
    }
    
    return head
}
```

**解析规则体**:

```go
func (p *Parser) parseBody() Body {
    var body Body
    
    p.expect(LBrace)
    
    for p.current.Type != RBrace {
        expr := p.parseExpression()
        body = append(body, expr)
        
        // 可选的分号或换行
        if p.current.Type == Semicolon {
            p.nextToken()
        }
    }
    
    p.expect(RBrace)
    
    return body
}
```

**解析表达式**:

```go
func (p *Parser) parseExpression() *Expr {
    expr := &Expr{
        Location: p.current.Location,
    }
    
    // 否定
    if p.current.Type == Not {
        p.nextToken()
        expr.Negated = true
        expr.Terms = p.parseExpression().Terms
        return expr
    }
    
    // 第一个项
    left := p.parseTerm()
    
    // 二元运算符
    if p.isBinaryOp(p.current.Type) {
        expr.Operator = p.current.Value
        p.nextToken()
        right := p.parseTerm()
        expr.Terms = []*Term{left, right}
    } else {
        // 单项表达式（如函数调用）
        expr.Terms = []*Term{left}
    }
    
    return expr
}
```

**解析项（Term）**:

```go
func (p *Parser) parseTerm() *Term {
    term := &Term{
        Location: p.current.Location,
    }
    
    switch p.current.Type {
    case Ident:
        // 可能是变量或引用
        term.Value = p.parseRef()
    
    case String:
        term.Value = StringTerm(p.current.Value)
        p.nextToken()
    
    case Number:
        num, _ := strconv.ParseFloat(p.current.Value, 64)
        term.Value = NumberTerm(num)
        p.nextToken()
    
    case Bool:
        term.Value = BooleanTerm(p.current.Value == "true")
        p.nextToken()
    
    case Null:
        term.Value = NullTerm{}
        p.nextToken()
    
    case LBrack:
        // 数组
        term.Value = p.parseArray()
    
    case LBrace:
        // 对象或集合
        term.Value = p.parseObjectOrSet()
    
    case LParen:
        // 分组
        p.nextToken()
        term = p.parseTerm()
        p.expect(RParen)
    
    default:
        p.error(fmt.Sprintf("unexpected token: %v", p.current))
    }
    
    return term
}
```

**解析数组**:

```go
func (p *Parser) parseArray() *Array {
    arr := &Array{}
    
    p.expect(LBrack)
    
    // 空数组
    if p.current.Type == RBrack {
        p.nextToken()
        return arr
    }
    
    // 检查是否是推导式
    if p.isComprehension() {
        return p.parseArrayComprehension()
    }
    
    // 普通数组
    for {
        arr.Terms = append(arr.Terms, p.parseTerm())
        
        if p.current.Type != Comma {
            break
        }
        p.nextToken()
    }
    
    p.expect(RBrack)
    
    return arr
}
```

---

## 4. AST构建

### 4.1 AST节点类型

**核心节点**:

```go
// Module: 模块（文件）
type Module struct {
    Package *Package
    Imports []*Import
    Rules   []*Rule
}

// Rule: 规则
type Rule struct {
    Head     *Head
    Body     Body
    Location *Location
}

// Head: 规则头
type Head struct {
    Name  *Term  // 规则名
    Key   *Term  // 部分规则的键
    Value *Term  // 规则值
}

// Body: 规则体（表达式列表）
type Body []*Expr

// Expr: 表达式
type Expr struct {
    Negated  bool
    Terms    []*Term
    With     []*With
    Location *Location
}

// Term: 项
type Term struct {
    Value    Value
    Location *Location
}

// Value: 值（接口，多种实现）
type Value interface {
    String() string
}

// 具体值类型
type (
    Null      struct{}
    Boolean   bool
    Number    json.Number
    String    string
    Var       string
    Ref       []*Term
    Array     []*Term
    Object    [][2]*Term
    Set       []*Term
    Call      []*Term
)
```

---

### 4.2 AST示例

**源码**:

```rego
package example

import future.keywords.if

allow if {
    input.method == "GET"
    user := data.users[input.user_id]
    "read" in user.permissions
}
```

**AST（简化表示）**:

```json
{
  "package": {
    "path": ["example"]
  },
  "imports": [
    {
      "path": ["future", "keywords", "if"]
    }
  ],
  "rules": [
    {
      "head": {
        "name": "allow"
      },
      "body": [
        {
          "terms": [
            {"ref": ["input", "method"]},
            {"string": "GET"}
          ],
          "operator": "=="
        },
        {
          "terms": [
            {"var": "user"},
            {
              "ref": [
                "data",
                "users",
                {"ref": ["input", "user_id"]}
              ]
            }
          ],
          "operator": ":="
        },
        {
          "terms": [
            {"string": "read"},
            {"ref": ["user", "permissions"]}
          ],
          "operator": "in"
        }
      ]
    }
  ]
}
```

---

## 5. 错误处理

### 5.1 错误类型

```go
type ParserError struct {
    Message  string
    Location *Location
}

func (e *ParserError) Error() string {
    return fmt.Sprintf("%s:%d:%d: %s",
        e.Location.File,
        e.Location.Row,
        e.Location.Col,
        e.Message)
}
```

### 5.2 错误报告

**示例**:

```rego
package example

allow if {
    input.method = "GET"   # 错误: 应该用 ==
}
```

**错误输出**:

```text
policy.rego:4:18: rego_parse_error: unexpected token
    input.method = "GET"
                 ^
expected one of: ==, !=, <, >, <=, >=, in
```

---

## 6. 实践示例

### 6.1 使用Go解析Rego

```go
package main

import (
    "fmt"
    "github.com/open-policy-agent/opa/ast"
)

func main() {
    source := `
        package example
        
        allow if {
            input.user == "admin"
        }
    `
    
    // 解析模块
    module, err := ast.ParseModule("policy.rego", source)
    if err != nil {
        panic(err)
    }
    
    // 访问AST
    fmt.Println("Package:", module.Package)
    fmt.Println("Rules:", len(module.Rules))
    
    for _, rule := range module.Rules {
        fmt.Printf("Rule: %s\n", rule.Head.Name)
        fmt.Printf("Body expressions: %d\n", len(rule.Body))
    }
}
```

---

### 6.2 自定义AST遍历

```go
// AST访问器
type Visitor struct{}

func (v *Visitor) Visit(node interface{}) {
    switch n := node.(type) {
    case *ast.Rule:
        fmt.Printf("Visiting rule: %s\n", n.Head.Name)
    case *ast.Expr:
        fmt.Printf("Visiting expression\n")
    case *ast.Term:
        fmt.Printf("Visiting term: %v\n", n.Value)
    }
}

// 遍历AST
func walkAST(module *ast.Module, visitor *Visitor) {
    for _, rule := range module.Rules {
        visitor.Visit(rule)
        for _, expr := range rule.Body {
            visitor.Visit(expr)
            for _, term := range expr.Terms {
                visitor.Visit(term)
            }
        }
    }
}
```

---

## 附录: 解析器性能

**基准测试**:

```text
BenchmarkParseLarge-8    500    3.2 ms/op    2.1 MB/s
```

**优化技巧**:

1. 使用字符串池（String Interning）
2. 缓存常用Token
3. 预分配AST节点切片
4. 避免不必要的内存分配

---

**下一篇**: [03.2-AST与IR](./03.2-AST与IR.md)  
**相关**: [02.1-Rego语法规范](../02-语言模型/02.1-Rego语法规范.md)
