# 编译器设计（Compiler Design）

> **更新日期**: 2025年10月21日  
> **OPA版本**: v0.60+  
> **实现语言**: Go  
> **参考**: <https://github.com/open-policy-agent/opa>

---

## 目录

- [编译器设计（Compiler Design）](#编译器设计compiler-design)
  - [目录](#目录)
  - [1. 编译器概述](#1-编译器概述)
    - [1.1 编译器架构](#11-编译器架构)
    - [1.2 编译阶段](#12-编译阶段)
    - [1.3 设计目标](#13-设计目标)
  - [2. 词法分析（Lexing）](#2-词法分析lexing)
    - [2.1 Token定义](#21-token定义)
    - [2.2 词法分析器实现](#22-词法分析器实现)
    - [2.3 错误处理](#23-错误处理)
  - [3. 语法分析（Parsing）](#3-语法分析parsing)
    - [3.1 文法定义](#31-文法定义)
    - [3.2 递归下降解析](#32-递归下降解析)
    - [3.3 运算符优先级](#33-运算符优先级)
  - [4. 语义分析](#4-语义分析)
    - [4.1 作用域分析](#41-作用域分析)
    - [4.2 类型检查](#42-类型检查)
    - [4.3 安全性检查](#43-安全性检查)
  - [5. 中间代码生成](#5-中间代码生成)
    - [5.1 IR生成策略](#51-ir生成策略)
    - [5.2 控制流图构建](#52-控制流图构建)
    - [5.3 SSA转换](#53-ssa转换)
  - [6. 优化](#6-优化)
    - [6.1 局部优化](#61-局部优化)
    - [6.2 全局优化](#62-全局优化)
    - [6.3 数据流分析](#63-数据流分析)
  - [7. 代码生成](#7-代码生成)
    - [7.1 字节码生成](#71-字节码生成)
    - [7.2 WASM生成](#72-wasm生成)
    - [7.3 原生代码生成](#73-原生代码生成)
  - [8. 错误处理与恢复](#8-错误处理与恢复)
    - [8.1 错误分类](#81-错误分类)
    - [8.2 错误恢复策略](#82-错误恢复策略)
    - [8.3 诊断信息](#83-诊断信息)
  - [9. 编译器优化技术](#9-编译器优化技术)
    - [9.1 部分求值](#91-部分求值)
    - [9.2 索引构建](#92-索引构建)
    - [9.3 规则重写](#93-规则重写)
  - [10. 实践案例](#10-实践案例)
    - [10.1 编译简单策略](#101-编译简单策略)
    - [10.2 编译复杂推导](#102-编译复杂推导)
    - [10.3 编译递归规则](#103-编译递归规则)
  - [附录A：编译器实现代码](#附录a编译器实现代码)
  - [附录B：优化Pass清单](#附录b优化pass清单)

---

## 1. 编译器概述

### 1.1 编译器架构

**OPA编译器结构**：

```text
┌──────────────────────────────────────────┐
│           Compiler Frontend              │
├──────────────────────────────────────────┤
│  Lexer → Parser → AST Builder            │
│    ↓                                     │
│  Semantic Analyzer                       │
│    ├── Scope Resolution                  │
│    ├── Type Checking                     │
│    └── Safety Checking                   │
└──────────────────┬───────────────────────┘
                   ↓
┌──────────────────────────────────────────┐
│         Compiler Middle-end              │
├──────────────────────────────────────────┤
│  IR Generator                            │
│    ↓                                     │
│  Optimizer                               │
│    ├── Constant Folding                  │
│    ├── Dead Code Elimination             │
│    ├── Inlining                          │
│    └── Partial Evaluation                │
└──────────────────┬───────────────────────┘
                   ↓
┌──────────────────────────────────────────┐
│           Compiler Backend               │
├──────────────────────────────────────────┤
│  Code Generator                          │
│    ├── Bytecode Generation               │
│    ├── WASM Generation                   │
│    └── Native Code Generation (JIT)      │
└──────────────────────────────────────────┘
```

### 1.2 编译阶段

**多Pass编译**：

```text
Pass 1: 词法与语法分析
  Input:  Rego source code
  Output: AST
  Time:   O(n) where n = source size

Pass 2: 语义分析
  Input:  AST
  Output: Annotated AST
  Time:   O(n) where n = AST size

Pass 3: IR生成
  Input:  Annotated AST
  Output: IR
  Time:   O(n)

Pass 4: 优化
  Input:  IR
  Output: Optimized IR
  Time:   O(n log n) ~ O(n²) depending on optimizations

Pass 5: 代码生成
  Input:  Optimized IR
  Output: Executable code
  Time:   O(n)
```

### 1.3 设计目标

**核心设计目标**：

```text
1. 正确性（Correctness）
   └── 严格遵循Rego语义规范

2. 性能（Performance）
   ├── 快速编译（< 100ms for典型策略）
   └── 高效执行（million decisions/sec）

3. 可调试性（Debuggability）
   ├── 精确的错误信息
   ├── 源码映射
   └── 符号信息保留

4. 可扩展性（Extensibility）
   ├── 支持新语言特性
   ├── 可插拔优化Pass
   └── 多后端支持
```

---

## 2. 词法分析（Lexing）

### 2.1 Token定义

**Token类型**：

```go
type TokenType int

const (
    // 关键字
    PACKAGE TokenType = iota
    IMPORT
    AS
    DEFAULT
    IF
    ELSE
    SOME
    IN
    EVERY
    NOT
    
    // 字面量
    IDENT      // 标识符
    NUMBER     // 123, 45.67
    STRING     // "hello"
    TRUE
    FALSE
    NULL
    
    // 运算符
    ASSIGN     // :=
    EQ         // =
    NEQ        // !=
    LT         // <
    LTE        // <=
    GT         // >
    GTE        // >=
    PLUS       // +
    MINUS      // -
    MULTIPLY   // *
    DIVIDE     // /
    
    // 分隔符
    LPAREN     // (
    RPAREN     // )
    LBRACK     // [
    RBRACK     // ]
    LBRACE     // {
    RBRACE     // }
    SEMICOLON  // ;
    DOT        // .
    COMMA      // ,
    
    // 特殊
    EOF
    ILLEGAL
)
```

### 2.2 词法分析器实现

**Lexer结构**：

```go
package compiler

import (
    "unicode"
    "unicode/utf8"
)

type Lexer struct {
    input   string
    pos     int    // 当前位置
    readPos int    // 下一个位置
    ch      rune   // 当前字符
    line    int
    column  int
}

func NewLexer(input string) *Lexer {
    l := &Lexer{
        input:  input,
        line:   1,
        column: 0,
    }
    l.readChar()
    return l
}

func (l *Lexer) readChar() {
    if l.readPos >= len(l.input) {
        l.ch = 0  // EOF
    } else {
        l.ch, _ = utf8.DecodeRuneInString(l.input[l.readPos:])
    }
    l.pos = l.readPos
    l.readPos++
    l.column++
    
    if l.ch == '\n' {
        l.line++
        l.column = 0
    }
}

func (l *Lexer) NextToken() Token {
    l.skipWhitespace()
    
    var tok Token
    tok.Line = l.line
    tok.Column = l.column
    
    switch l.ch {
    case '=':
        if l.peekChar() == '=' {
            l.readChar()
            tok.Type = EQ
            tok.Literal = "=="
        } else {
            tok.Type = ASSIGN
            tok.Literal = "="
        }
    case '+':
        tok.Type = PLUS
        tok.Literal = string(l.ch)
    case '-':
        tok.Type = MINUS
        tok.Literal = string(l.ch)
    case '(':
        tok.Type = LPAREN
        tok.Literal = string(l.ch)
    case ')':
        tok.Type = RPAREN
        tok.Literal = string(l.ch)
    case '"':
        tok.Type = STRING
        tok.Literal = l.readString()
    case 0:
        tok.Type = EOF
        tok.Literal = ""
    default:
        if isLetter(l.ch) {
            tok.Literal = l.readIdentifier()
            tok.Type = lookupIdent(tok.Literal)
            return tok
        } else if isDigit(l.ch) {
            tok.Type = NUMBER
            tok.Literal = l.readNumber()
            return tok
        } else {
            tok.Type = ILLEGAL
            tok.Literal = string(l.ch)
        }
    }
    
    l.readChar()
    return tok
}

func (l *Lexer) readIdentifier() string {
    position := l.pos
    for isLetter(l.ch) || isDigit(l.ch) || l.ch == '_' {
        l.readChar()
    }
    return l.input[position:l.pos]
}

func (l *Lexer) readNumber() string {
    position := l.pos
    for isDigit(l.ch) {
        l.readChar()
    }
    if l.ch == '.' {
        l.readChar()
        for isDigit(l.ch) {
            l.readChar()
        }
    }
    return l.input[position:l.pos]
}

func (l *Lexer) readString() string {
    position := l.pos + 1
    for {
        l.readChar()
        if l.ch == '"' || l.ch == 0 {
            break
        }
        if l.ch == '\\' {
            l.readChar()  // Skip escaped character
        }
    }
    return l.input[position:l.pos]
}
```

### 2.3 错误处理

**词法错误**：

```go
type LexError struct {
    Line    int
    Column  int
    Message string
}

func (l *Lexer) error(msg string) *LexError {
    return &LexError{
        Line:    l.line,
        Column:  l.column,
        Message: msg,
    }
}

// 示例：未闭合的字符串
func (l *Lexer) readString() (string, error) {
    position := l.pos + 1
    for {
        l.readChar()
        if l.ch == 0 {
            return "", l.error("未闭合的字符串")
        }
        if l.ch == '"' {
            break
        }
    }
    return l.input[position:l.pos], nil
}
```

---

## 3. 语法分析（Parsing）

### 3.1 文法定义

**Rego文法（简化EBNF）**：

```ebnf
module       = package imports rules

package      = "package" ref

imports      = { import }
import       = "import" ref [ "as" var ]

rules        = { rule }
rule         = [ "default" ] head [ "if" "{" body "}" ] [ else_clause ]

head         = ref [ "=" term ] [ "{" term "}" ]

body         = expr { ( ";" | newline ) expr }

expr         = term | expr infix_op term | prefix_op expr

term         = ref | var | scalar | array | object | set | comprehension | call

infix_op     = "==" | "!=" | "<" | "<=" | ">" | ">=" | "+" | "-" | "*" | "/" | "&" | "|"
prefix_op    = "not"

ref          = var { ref_arg }
ref_arg      = dot_arg | bracket_arg
dot_arg      = "." var
bracket_arg  = "[" term "]"

array        = "[" [ term { "," term } ] "]"
object       = "{" [ object_item { "," object_item } ] "}"
object_item  = term ":" term

set          = "{" term { "," term } "}"
```

### 3.2 递归下降解析

**Parser实现**：

```go
type Parser struct {
    lexer    *Lexer
    curToken Token
    peekToken Token
    errors   []error
}

func NewParser(lexer *Lexer) *Parser {
    p := &Parser{lexer: lexer}
    p.nextToken()
    p.nextToken()
    return p
}

func (p *Parser) ParseModule() (*ast.Module, error) {
    module := &ast.Module{}
    
    // 解析package
    if !p.expectToken(PACKAGE) {
        return nil, p.error("expected 'package'")
    }
    module.Package = p.parsePackage()
    
    // 解析imports
    for p.curTokenIs(IMPORT) {
        module.Imports = append(module.Imports, p.parseImport())
    }
    
    // 解析rules
    for !p.curTokenIs(EOF) {
        rule := p.parseRule()
        if rule != nil {
            module.Rules = append(module.Rules, rule)
        }
        p.nextToken()
    }
    
    if len(p.errors) > 0 {
        return nil, fmt.Errorf("parsing errors: %v", p.errors)
    }
    
    return module, nil
}

func (p *Parser) parseRule() *ast.Rule {
    rule := &ast.Rule{}
    
    // Default规则
    if p.curTokenIs(DEFAULT) {
        rule.Default = true
        p.nextToken()
    }
    
    // 解析Head
    rule.Head = p.parseHead()
    
    // 解析Body (if { ... })
    if p.curTokenIs(IF) {
        p.nextToken()
        p.expectToken(LBRACE)
        rule.Body = p.parseBody()
        p.expectToken(RBRACE)
    }
    
    // Else子句
    if p.curTokenIs(ELSE) {
        rule.Else = p.parseElse()
    }
    
    return rule
}

func (p *Parser) parseExpr() ast.Expr {
    return p.parseInfixExpr(p.parsePrefixExpr(), 0)
}

func (p *Parser) parseInfixExpr(left ast.Expr, precedence int) ast.Expr {
    for p.peekToken.Type.IsInfix() && p.peekPrecedence() > precedence {
        p.nextToken()
        op := p.curToken
        p.nextToken()
        right := p.parsePrefixExpr()
        right = p.parseInfixExpr(right, p.curPrecedence())
        
        left = &ast.InfixExpr{
            Left:     left,
            Operator: op.Literal,
            Right:    right,
        }
    }
    return left
}
```

### 3.3 运算符优先级

**优先级表**：

```go
const (
    _ int = iota
    LOWEST
    LOGIC_OR      // |
    LOGIC_AND     // &
    EQUALS        // ==, !=
    LESSGREATER   // <, <=, >, >=
    SUM           // +, -
    PRODUCT       // *, /
    PREFIX        // not
    CALL          // function()
)

var precedences = map[TokenType]int{
    OR:       LOGIC_OR,
    AND:      LOGIC_AND,
    EQ:       EQUALS,
    NEQ:      EQUALS,
    LT:       LESSGREATER,
    LTE:      LESSGREATER,
    GT:       LESSGREATER,
    GTE:      LESSGREATER,
    PLUS:     SUM,
    MINUS:    SUM,
    MULTIPLY: PRODUCT,
    DIVIDE:   PRODUCT,
    NOT:      PREFIX,
}
```

---

## 4. 语义分析

### 4.1 作用域分析

**作用域解析器**：

```go
type ScopeAnalyzer struct {
    scopes  []*Scope
    errors  []error
}

type Scope struct {
    parent  *Scope
    symbols map[string]*Symbol
}

func (sa *ScopeAnalyzer) EnterScope() {
    scope := &Scope{
        parent:  sa.currentScope(),
        symbols: make(map[string]*Symbol),
    }
    sa.scopes = append(sa.scopes, scope)
}

func (sa *ScopeAnalyzer) ExitScope() {
    sa.scopes = sa.scopes[:len(sa.scopes)-1]
}

func (sa *ScopeAnalyzer) Declare(name string, symbol *Symbol) error {
    scope := sa.currentScope()
    if _, exists := scope.symbols[name]; exists {
        return fmt.Errorf("重复定义: %s", name)
    }
    scope.symbols[name] = symbol
    return nil
}

func (sa *ScopeAnalyzer) Resolve(name string) (*Symbol, error) {
    for scope := sa.currentScope(); scope != nil; scope = scope.parent {
        if symbol, ok := scope.symbols[name]; ok {
            return symbol, nil
        }
    }
    return nil, fmt.Errorf("未定义的变量: %s", name)
}
```

### 4.2 类型检查

**类型检查器**：

```go
type TypeChecker struct {
    env    *TypeEnvironment
    errors []error
}

func (tc *TypeChecker) Check(module *ast.Module) error {
    for _, rule := range module.Rules {
        tc.checkRule(rule)
    }
    return tc.wrapErrors()
}

func (tc *TypeChecker) checkRule(rule *ast.Rule) {
    // 推断Head类型
    headType := tc.inferType(rule.Head.Value)
    
    // 检查Body
    for _, expr := range rule.Body {
        exprType := tc.inferType(expr)
        
        // Body表达式必须返回布尔值
        if !exprType.AssignableTo(TypeBoolean) {
            tc.errors = append(tc.errors,
                fmt.Errorf("表达式必须返回布尔值，实际: %v", exprType))
        }
    }
    
    // 检查Default值类型
    if rule.Default && rule.Head.Value != nil {
        defaultType := tc.inferType(rule.Head.Value)
        if !defaultType.AssignableTo(headType) {
            tc.errors = append(tc.errors,
                fmt.Errorf("default值类型不匹配"))
        }
    }
}

func (tc *TypeChecker) inferType(term ast.Term) Type {
    switch t := term.(type) {
    case *ast.Number:
        return TypeNumber
    case *ast.String:
        return TypeString
    case *ast.Boolean:
        return TypeBoolean
    case *ast.Array:
        elemType := tc.inferArrayElementType(t)
        return &ArrayType{Element: elemType}
    case *ast.Object:
        keyType, valType := tc.inferObjectTypes(t)
        return &ObjectType{Key: keyType, Value: valType}
    case *ast.Var:
        return tc.env.Lookup(t.Value)
    case *ast.Ref:
        return tc.inferRefType(t)
    }
    return TypeAny
}
```

### 4.3 安全性检查

**安全性分析器**：

```go
type SafetyChecker struct {
    errors []error
}

func (sc *SafetyChecker) Check(rule *ast.Rule) error {
    // 收集所有变量
    vars := collectVars(rule)
    
    // 检查否定中的变量
    sc.checkNegation(rule, vars)
    
    // 检查推导中的变量
    sc.checkComprehensions(rule, vars)
    
    return sc.wrapErrors()
}

func (sc *SafetyChecker) checkNegation(rule *ast.Rule, safeVars map[string]bool) {
    for _, expr := range rule.Body {
        if not, ok := expr.(*ast.NotExpr); ok {
            // 否定中的所有变量必须在否定前已绑定
            notVars := collectVars(not.Expr)
            for v := range notVars {
                if !safeVars[v] {
                    sc.errors = append(sc.errors,
                        fmt.Errorf("否定中的未绑定变量: %s", v))
                }
            }
        }
    }
}
```

---

## 5. 中间代码生成

### 5.1 IR生成策略

**规则到IR转换**：

```go
type IRBuilder struct {
    nextBlockID  int
    nextRegister int
}

func (ib *IRBuilder) BuildRule(rule *ast.Rule) *IR {
    ir := &IR{
        Name:   rule.Head.Name,
        Blocks: make([]*Block, 0),
    }
    
    // 创建入口块
    entry := ib.newBlock("entry")
    ir.Blocks = append(ir.Blocks, entry)
    
    // 编译Body
    for _, expr := range rule.Body {
        ib.compileExpr(expr, entry)
    }
    
    // 编译Head
    result := ib.compileHead(rule.Head, entry)
    
    // 返回结果
    entry.addInstr(&Return{Value: result})
    
    return ir
}

func (ib *IRBuilder) compileExpr(expr ast.Expr, block *Block) Register {
    switch e := expr.(type) {
    case *ast.InfixExpr:
        return ib.compileInfix(e, block)
    case *ast.CallExpr:
        return ib.compileCall(e, block)
    case *ast.RefExpr:
        return ib.compileRef(e, block)
    }
    return 0
}

func (ib *IRBuilder) compileInfix(expr *ast.InfixExpr, block *Block) Register {
    left := ib.compileExpr(expr.Left, block)
    right := ib.compileExpr(expr.Right, block)
    
    result := ib.newRegister()
    
    switch expr.Operator {
    case "+":
        block.addInstr(&Add{Dst: result, Src1: left, Src2: right})
    case "==":
        block.addInstr(&Eq{Dst: result, Src1: left, Src2: right})
    // ... 其他运算符
    }
    
    return result
}
```

### 5.2 控制流图构建

**CFG（Control Flow Graph）**：

```go
type CFG struct {
    Blocks []*Block
    Entry  *Block
    Exit   *Block
}

type Block struct {
    ID           int
    Label        string
    Instructions []Instruction
    Successors   []*Block
    Predecessors []*Block
}

func BuildCFG(ir *IR) *CFG {
    cfg := &CFG{
        Blocks: ir.Blocks,
        Entry:  ir.Blocks[0],
    }
    
    // 构建边
    for _, block := range ir.Blocks {
        lastInstr := block.Instructions[len(block.Instructions)-1]
        
        switch instr := lastInstr.(type) {
        case *Jump:
            target := findBlock(ir.Blocks, instr.Label)
            block.Successors = append(block.Successors, target)
            target.Predecessors = append(target.Predecessors, block)
            
        case *Branch:
            trueTarget := findBlock(ir.Blocks, instr.TrueLabel)
            falseTarget := findBlock(ir.Blocks, instr.FalseLabel)
            
            block.Successors = append(block.Successors, trueTarget, falseTarget)
            trueTarget.Predecessors = append(trueTarget.Predecessors, block)
            falseTarget.Predecessors = append(falseTarget.Predecessors, block)
        }
    }
    
    return cfg
}
```

### 5.3 SSA转换

**SSA（Static Single Assignment）形式**：

```text
// 原始IR
x = 1
x = x + 2

// SSA形式
x_1 = 1
x_2 = x_1 + 2
```

**Φ函数插入**：

```go
func ConvertToSSA(cfg *CFG) {
    // 1. 插入Φ函数
    insertPhiFunctions(cfg)
    
    // 2. 变量重命名
    renameVariables(cfg)
}

func insertPhiFunctions(cfg *CFG) {
    for _, block := range cfg.Blocks {
        if len(block.Predecessors) > 1 {
            // 为所有变量插入Φ函数
            vars := collectDefinedVars(block)
            for _, v := range vars {
                phi := &Phi{
                    Dst:     newSSAVar(v),
                    Sources: make([]Register, len(block.Predecessors)),
                }
                block.prependInstr(phi)
            }
        }
    }
}
```

---

## 6. 优化

### 6.1 局部优化

**常量折叠**：

```go
func ConstantFolding(block *Block) {
    for i, instr := range block.Instructions {
        switch op := instr.(type) {
        case *Add:
            if isConst(op.Src1) && isConst(op.Src2) {
                val := getConstValue(op.Src1) + getConstValue(op.Src2)
                block.Instructions[i] = &LoadConst{Dst: op.Dst, Value: val}
            }
        case *Mul:
            if isConst(op.Src1) && isConst(op.Src2) {
                val := getConstValue(op.Src1) * getConstValue(op.Src2)
                block.Instructions[i] = &LoadConst{Dst: op.Dst, Value: val}
            }
        }
    }
}
```

**强度削减**：

```go
// x * 2  →  x + x
// x * 4  →  x << 2
func StrengthReduction(block *Block) {
    for i, instr := range block.Instructions {
        if mul, ok := instr.(*Mul); ok {
            if isConst(mul.Src2) {
                val := getConstValue(mul.Src2)
                if isPowerOfTwo(val) {
                    // 替换为移位
                    shift := log2(val)
                    block.Instructions[i] = &Shl{
                        Dst:  mul.Dst,
                        Src:  mul.Src1,
                        Amt:  shift,
                    }
                }
            }
        }
    }
}
```

### 6.2 全局优化

**死代码消除**：

```go
func DeadCodeElimination(cfg *CFG) {
    // 1. 标记活跃指令
    alive := make(map[Instruction]bool)
    worklist := []Instruction{}
    
    // 初始化：返回语句和有副作用的指令
    for _, block := range cfg.Blocks {
        for _, instr := range block.Instructions {
            if isCritical(instr) {
                alive[instr] = true
                worklist = append(worklist, instr)
            }
        }
    }
    
    // 2. 反向传播
    for len(worklist) > 0 {
        instr := worklist[0]
        worklist = worklist[1:]
        
        for _, operand := range instr.Operands() {
            defInstr := findDefinition(operand)
            if !alive[defInstr] {
                alive[defInstr] = true
                worklist = append(worklist, defInstr)
            }
        }
    }
    
    // 3. 删除死代码
    for _, block := range cfg.Blocks {
        newInstrs := []Instruction{}
        for _, instr := range block.Instructions {
            if alive[instr] {
                newInstrs = append(newInstrs, instr)
            }
        }
        block.Instructions = newInstrs
    }
}
```

### 6.3 数据流分析

**到达定义分析**：

```go
type ReachingDefs struct {
    Gen  map[*Block]Set  // 块生成的定义
    Kill map[*Block]Set  // 块杀死的定义
    In   map[*Block]Set  // 块入口的定义
    Out  map[*Block]Set  // 块出口的定义
}

func ComputeReachingDefs(cfg *CFG) *ReachingDefs {
    rd := &ReachingDefs{
        Gen:  make(map[*Block]Set),
        Kill: make(map[*Block]Set),
        In:   make(map[*Block]Set),
        Out:  make(map[*Block]Set),
    }
    
    // 计算Gen和Kill
    for _, block := range cfg.Blocks {
        rd.Gen[block] = computeGen(block)
        rd.Kill[block] = computeKill(block)
    }
    
    // 迭代求解
    changed := true
    for changed {
        changed = false
        for _, block := range cfg.Blocks {
            // In[B] = ∪ Out[P] for P in predecessors(B)
            newIn := NewSet()
            for _, pred := range block.Predecessors {
                newIn = newIn.Union(rd.Out[pred])
            }
            
            // Out[B] = Gen[B] ∪ (In[B] - Kill[B])
            newOut := rd.Gen[block].Union(
                newIn.Difference(rd.Kill[block]),
            )
            
            if !newIn.Equals(rd.In[block]) || !newOut.Equals(rd.Out[block]) {
                changed = true
                rd.In[block] = newIn
                rd.Out[block] = newOut
            }
        }
    }
    
    return rd
}
```

---

## 7. 代码生成

### 7.1 字节码生成

**字节码定义**：

```go
type Opcode byte

const (
    OP_LOAD_CONST Opcode = iota
    OP_LOAD_VAR
    OP_STORE_VAR
    OP_ADD
    OP_SUB
    OP_MUL
    OP_EQ
    OP_JUMP
    OP_JUMP_IF_FALSE
    OP_CALL
    OP_RETURN
)

type BytecodeGenerator struct {
    code []byte
}

func (bg *BytecodeGenerator) Generate(ir *IR) []byte {
    for _, block := range ir.Blocks {
        for _, instr := range block.Instructions {
            bg.emitInstruction(instr)
        }
    }
    return bg.code
}

func (bg *BytecodeGenerator) emitInstruction(instr Instruction) {
    switch i := instr.(type) {
    case *LoadConst:
        bg.emit(OP_LOAD_CONST)
        bg.emitInt(i.Dst)
        bg.emitValue(i.Value)
    case *Add:
        bg.emit(OP_ADD)
        bg.emitInt(i.Dst)
        bg.emitInt(i.Src1)
        bg.emitInt(i.Src2)
    case *Return:
        bg.emit(OP_RETURN)
        bg.emitInt(i.Value)
    }
}
```

### 7.2 WASM生成

**WASM代码生成器**：

```go
func GenerateWASM(ir *IR) []byte {
    module := &wasm.Module{
        Types: []wasm.FunctionType{
            {
                Params:  []wasm.ValueType{wasm.I32},
                Results: []wasm.ValueType{wasm.I32},
            },
        },
        Functions: []wasm.Function{},
        Exports: []wasm.Export{
            {
                Name: "eval",
                Kind: wasm.ExternalFunction,
                Index: 0,
            },
        },
    }
    
    // 生成函数体
    funcBody := generateWASMFunction(ir)
    module.Functions = append(module.Functions, funcBody)
    
    // 编码为字节
    return wasm.Encode(module)
}

func generateWASMFunction(ir *IR) wasm.Function {
    code := []byte{}
    
    for _, block := range ir.Blocks {
        for _, instr := range block.Instructions {
            switch i := instr.(type) {
            case *LoadConst:
                code = append(code, wasm.I32Const)
                code = appendLEB128(code, i.Value)
            case *Add:
                code = append(code, wasm.LocalGet, byte(i.Src1))
                code = append(code, wasm.LocalGet, byte(i.Src2))
                code = append(code, wasm.I32Add)
                code = append(code, wasm.LocalSet, byte(i.Dst))
            case *Return:
                code = append(code, wasm.LocalGet, byte(i.Value))
                code = append(code, wasm.Return)
            }
        }
    }
    
    return wasm.Function{Code: code}
}
```

### 7.3 原生代码生成

**JIT编译（概念）**：

```go
// 使用 LLVM 或 cranelift 生成原生代码
func CompileToNative(ir *IR) (*NativeCode, error) {
    // 1. IR → LLVM IR
    llvmModule := convertToLLVM(ir)
    
    // 2. LLVM优化
    runLLVMOptimizations(llvmModule)
    
    // 3. 生成机器码
    machineCode := llvmModule.Compile()
    
    // 4. JIT执行
    return &NativeCode{
        Code:   machineCode,
        Entry:  machineCode.GetFunction("eval"),
    }, nil
}
```

---

## 8. 错误处理与恢复

### 8.1 错误分类

**错误类型**：

```go
type CompileError interface {
    error
    Location() *Location
    Level() ErrorLevel
}

type ErrorLevel int

const (
    ERROR   ErrorLevel = iota  // 致命错误
    WARNING                    // 警告
    INFO                       // 信息
)

type SyntaxError struct {
    Loc     *Location
    Message string
}

type TypeError struct {
    Loc      *Location
    Expected Type
    Got      Type
}

type SafetyError struct {
    Loc  *Location
    Var  string
    Kind SafetyViolation
}
```

### 8.2 错误恢复策略

**Panic模式恢复**：

```go
func (p *Parser) synchronize() {
    p.panicMode = false
    
    for !p.curTokenIs(EOF) {
        if p.peekTokenIs(SEMICOLON) {
            p.nextToken()
            return
        }
        
        switch p.curToken.Type {
        case PACKAGE, IMPORT, DEFAULT:
            return
        }
        
        p.nextToken()
    }
}

func (p *Parser) parseRule() *ast.Rule {
    defer func() {
        if r := recover(); r != nil {
            p.errors = append(p.errors, r.(error))
            p.synchronize()
        }
    }()
    
    // 解析逻辑...
}
```

### 8.3 诊断信息

**错误报告格式**：

```go
func FormatError(err CompileError, source string) string {
    loc := err.Location()
    lines := strings.Split(source, "\n")
    
    // 构建错误信息
    var buf bytes.Buffer
    
    // 文件位置
    fmt.Fprintf(&buf, "%s:%d:%d: %s\n",
        loc.File, loc.Row, loc.Col, err.Error())
    
    // 源代码行
    if loc.Row > 0 && loc.Row <= len(lines) {
        line := lines[loc.Row-1]
        fmt.Fprintf(&buf, "%5d | %s\n", loc.Row, line)
        
        // 错误指示符
        fmt.Fprintf(&buf, "      | %s^\n",
            strings.Repeat(" ", loc.Col-1))
    }
    
    return buf.String()
}
```

---

## 9. 编译器优化技术

### 9.1 部分求值

**部分求值优化**：

```go
func PartialEval(module *ast.Module, knownData map[string]interface{}) *ast.Module {
    optimizer := &PartialEvaluator{
        data: knownData,
    }
    
    optimized := &ast.Module{
        Package: module.Package,
        Imports: module.Imports,
    }
    
    for _, rule := range module.Rules {
        optRule := optimizer.evalRule(rule)
        if optRule != nil {
            optimized.Rules = append(optimized.Rules, optRule)
        }
    }
    
    return optimized
}

func (pe *PartialEvaluator) evalRule(rule *ast.Rule) *ast.Rule {
    newBody := []ast.Expr{}
    
    for _, expr := range rule.Body {
        // 尝试求值表达式
        if result, ok := pe.tryEval(expr); ok {
            if !result {
                // 表达式为false，整个规则失败
                return nil
            }
            // 表达式为true，可以删除
            continue
        }
        newBody = append(newBody, expr)
    }
    
    return &ast.Rule{
        Head: rule.Head,
        Body: newBody,
    }
}
```

### 9.2 索引构建

**自动索引生成**：

```go
type IndexBuilder struct {
    indexes map[string]*Index
}

func (ib *IndexBuilder) BuildIndexes(module *ast.Module) {
    // 分析访问模式
    patterns := ib.analyzeAccessPatterns(module)
    
    // 为频繁访问的路径构建索引
    for _, pattern := range patterns {
        if pattern.Frequency > THRESHOLD {
            index := ib.createIndex(pattern)
            ib.indexes[pattern.Path] = index
        }
    }
}

type Index struct {
    Path     string
    KeyType  Type
    ValueMap map[interface{}]interface{}
}

func (ib *IndexBuilder) createIndex(pattern AccessPattern) *Index {
    index := &Index{
        Path:     pattern.Path,
        KeyType:  pattern.KeyType,
        ValueMap: make(map[interface{}]interface{}),
    }
    
    // 构建索引映射
    for _, item := range pattern.Data {
        key := extractKey(item, pattern.KeyPath)
        index.ValueMap[key] = item
    }
    
    return index
}
```

### 9.3 规则重写

**规则简化**：

```go
// 合并相同条件的规则
func MergeRules(rules []*ast.Rule) []*ast.Rule {
    groups := groupByConditions(rules)
    merged := []*ast.Rule{}
    
    for _, group := range groups {
        if len(group) > 1 {
            // 合并为单个规则
            merged = append(merged, mergeGroup(group))
        } else {
            merged = append(merged, group[0])
        }
    }
    
    return merged
}

// 示例：
// rule1 if { x > 10; y == 5 }
// rule1 if { x > 10; y == 6 }
// →
// rule1 if { x > 10; y in {5, 6} }
```

---

## 10. 实践案例

### 10.1 编译简单策略

**输入策略**：

```rego
package authz

allow if {
    input.user == "admin"
}
```

**编译步骤**：

```text
1. Lexer输出:
   PACKAGE, IDENT("authz"), IDENT("allow"), IF, ...

2. Parser输出AST:
   Module(
     Package("authz"),
     Rule(
       Head("allow"),
       Body([
         Expr(Eq(Ref("input.user"), String("admin")))
       ])
     )
   )

3. IR生成:
   function authz.allow:
     %1 = load input
     %2 = get %1, "user"
     %3 = eq %2, "admin"
     return %3

4. 优化后IR:
   (无变化)

5. 字节码:
   LOAD_VAR input
   GET_FIELD "user"
   LOAD_CONST "admin"
   EQ
   RETURN
```

### 10.2 编译复杂推导

**输入策略**：

```rego
admins := [u | 
    some u in data.users
    u.role == "admin"
]
```

**IR生成**：

```text
function authz.admins:
  %result = make_array []
  %users = load data.users
  %iter = scan %users
  
loop:
  %u = next %iter
  branch_exhausted %iter, done
  
  %role = get %u, "role"
  %cond = eq %role, "admin"
  branch_not %cond, loop
  
  append %result, %u
  jump loop
  
done:
  return %result
```

### 10.3 编译递归规则

**输入策略**：

```rego
reachable[y] if {
    edge[input.start][y]
}

reachable[z] if {
    reachable[y]
    edge[y][z]
}
```

**编译策略**：

```text
1. 检测递归
2. 生成fixpoint迭代代码
3. 使用工作表算法

伪代码:
worklist = {input.start}
result = set()

while worklist not empty:
    x = worklist.pop()
    if x in result:
        continue
    result.add(x)
    
    for y in edge[x]:
        worklist.add(y)

return result
```

---

## 附录A：编译器实现代码

**完整编译器框架**：

```go
package compiler

type Compiler struct {
    lexer      *Lexer
    parser     *Parser
    analyzer   *SemanticAnalyzer
    irBuilder  *IRBuilder
    optimizer  *Optimizer
    codegen    *CodeGenerator
}

func NewCompiler() *Compiler {
    return &Compiler{
        analyzer:  NewSemanticAnalyzer(),
        irBuilder: NewIRBuilder(),
        optimizer: NewOptimizer(),
        codegen:   NewCodeGenerator(),
    }
}

func (c *Compiler) Compile(source string) (*CompiledModule, error) {
    // 1. 词法和语法分析
    c.lexer = NewLexer(source)
    c.parser = NewParser(c.lexer)
    ast, err := c.parser.Parse()
    if err != nil {
        return nil, err
    }
    
    // 2. 语义分析
    if err := c.analyzer.Analyze(ast); err != nil {
        return nil, err
    }
    
    // 3. IR生成
    ir := c.irBuilder.Build(ast)
    
    // 4. 优化
    optimizedIR := c.optimizer.Optimize(ir)
    
    // 5. 代码生成
    code := c.codegen.Generate(optimizedIR)
    
    return &CompiledModule{
        AST:  ast,
        IR:   optimizedIR,
        Code: code,
    }, nil
}
```

---

## 附录B：优化Pass清单

**推荐优化Pass顺序**：

```text
1. 常量传播 (Constant Propagation)
2. 常量折叠 (Constant Folding)
3. 死代码消除 (Dead Code Elimination)
4. 公共子表达式消除 (Common Subexpression Elimination)
5. 复写传播 (Copy Propagation)
6. 强度削减 (Strength Reduction)
7. 循环不变代码外提 (Loop Invariant Code Motion)
8. 函数内联 (Inlining)
9. 尾调用优化 (Tail Call Optimization)
10. 部分求值 (Partial Evaluation)
```

---

**相关文档**：

- [词法分析与语法解析](./03.1-词法分析与语法解析.md)
- [AST与IR](./03.2-AST与IR.md)
- [Top-Down求值器](./03.4-Top-Down求值器.md)

**参考资源**：

- Dragon Book: "Compilers: Principles, Techniques, and Tools"
- LLVM Documentation: <https://llvm.org/docs/>
- OPA Compiler Source: <https://github.com/open-policy-agent/opa/tree/main/ast>

